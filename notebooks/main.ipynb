{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üöÄ Setup Neo4j con Docker\n",
    "\n",
    "## Cosa stiamo facendo?\n",
    "- I tuoi **CSV** contengono dati tabellari di AdventureWorks\n",
    "- Li caricheremo in **Neo4j** (database a grafo) per creare la knowledge base\n",
    "- Neo4j ci permette di collegare prodotti, documenti e annotazioni con relazioni semantiche\n",
    "\n",
    "## 1. Avvia Neo4j con Docker\n",
    "\n",
    "Apri un terminale ed esegui:\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "    -p 7474:7474 -p 7687:7687 \\\n",
    "    -d \\\n",
    "    -v neo4j_data:/data \\\n",
    "    -v neo4j_logs:/logs \\\n",
    "    -v neo4j_import:/var/lib/neo4j/import \\\n",
    "    -v neo4j_plugins:/plugins \\\n",
    "    --env NEO4J_AUTH=neo4j/password \\\n",
    "    neo4j:latest\n",
    "```\n",
    "\n",
    "## 2. Verifica che funzioni\n",
    "\n",
    "- Apri il browser su: **http://localhost:7474**\n",
    "- Login: `neo4j` / `password`\n",
    "- Dovresti vedere l'interfaccia Neo4j Browser\n",
    "\n",
    "## 3. Esegui le celle sotto\n",
    "\n",
    "Una volta che Neo4j √® avviato, puoi eseguire le celle Python che caricheranno i dati CSV nel grafo!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2907671a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerie importate con successo per la creazione del grafo di conoscenza\n"
     ]
    }
   ],
   "source": [
    "# Importazione delle librerie necessarie per il grafo di conoscenza\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from neo4j import GraphDatabase\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Librerie importate con successo per la creazione del grafo di conoscenza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a17e8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe KnowledgeGraphBuilder definita\n"
     ]
    }
   ],
   "source": [
    "# Configurazione della connessione a Neo4j\n",
    "# Avviarlo con Docker: docker run -p 7474:7474 -p 7687:7687 neo4j:latest\n",
    "\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\"  # Cambia con la tua password\n",
    "\n",
    "\n",
    "class KnowledgeGraphBuilder:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def clear_database(self):\n",
    "        \"\"\"Pulisce il database per ricominciare da capo\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            logger.info(\"Database pulito\")\n",
    "\n",
    "    def create_indexes(self):\n",
    "        \"\"\"Crea gli indici per migliorare le performance\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Indici per i prodotti\n",
    "            session.run(\n",
    "                \"CREATE INDEX product_id IF NOT EXISTS FOR (p:Product) ON (p.product_id)\"\n",
    "            )\n",
    "            session.run(\n",
    "                \"CREATE INDEX product_name IF NOT EXISTS FOR (p:Product) ON (p.name)\"\n",
    "            )\n",
    "\n",
    "            # Indici per i documenti\n",
    "            session.run(\n",
    "                \"CREATE INDEX document_filename IF NOT EXISTS FOR (d:Document) ON (d.filename)\"\n",
    "            )\n",
    "\n",
    "            # Indici per le annotazioni\n",
    "            session.run(\n",
    "                \"CREATE INDEX annotation_filename IF NOT EXISTS FOR (a:Annotation) ON (a.filename)\"\n",
    "            )\n",
    "\n",
    "            logger.info(\"Indici creati\")\n",
    "\n",
    "\n",
    "print(\"Classe KnowledgeGraphBuilder definita\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da168f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Caricati 100 prodotti\n",
      "INFO:__main__:Caricate 41 categorie\n",
      "INFO:__main__:Caricate 100 descrizioni\n",
      "INFO:__main__:Caricati 100 modelli\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nPrimi 3 prodotti:\n",
      "   ProductID                       Name  ProductCategoryID  ProductModelID\n",
      "0        680  HL Road Frame - Black, 58                 18               6\n",
      "1        706    HL Road Frame - Red, 58                 18               6\n",
      "2        707      Sport-100 Helmet, Red                 35              33\n",
      "\\nPrime 5 categorie:\n",
      "   ProductCategoryID  ParentProductCategoryID            Name\n",
      "0                  1                      NaN           Bikes\n",
      "1                  2                      NaN      Components\n",
      "2                  3                      NaN        Clothing\n",
      "3                  4                      NaN     Accessories\n",
      "4                  5                      1.0  Mountain Bikes\n"
     ]
    }
   ],
   "source": [
    "# Caricamento e preparazione dei dati CSV\n",
    "def load_csv_data():\n",
    "    \"\"\"Carica tutti i file CSV da AdventureWorks\"\"\"\n",
    "    data_path = Path(\"../data\")\n",
    "\n",
    "    # Leggiamo i CSV principali\n",
    "    products_df = pd.read_csv(data_path / \"Product.csv\", sep=\";\")\n",
    "    categories_df = pd.read_csv(data_path / \"ProductCategory.csv\", sep=\";\")\n",
    "    descriptions_df = pd.read_csv(data_path / \"ProductDescription.csv\", sep=\";\")\n",
    "    models_df = pd.read_csv(data_path / \"ProductModel.csv\", sep=\";\")\n",
    "\n",
    "    logger.info(f\"Caricati {len(products_df)} prodotti\")\n",
    "    logger.info(f\"Caricate {len(categories_df)} categorie\")\n",
    "    logger.info(f\"Caricate {len(descriptions_df)} descrizioni\")\n",
    "    logger.info(f\"Caricati {len(models_df)} modelli\")\n",
    "\n",
    "    return {\n",
    "        \"products\": products_df,\n",
    "        \"categories\": categories_df,\n",
    "        \"descriptions\": descriptions_df,\n",
    "        \"models\": models_df,\n",
    "    }\n",
    "\n",
    "\n",
    "# Carica i dati\n",
    "csv_data = load_csv_data()\n",
    "\n",
    "# Esplora i primi record per vedere la struttura\n",
    "print(\"\\\\nPrimi 3 prodotti:\")\n",
    "print(\n",
    "    csv_data[\"products\"][\n",
    "        [\"ProductID\", \"Name\", \"ProductCategoryID\", \"ProductModelID\"]\n",
    "    ].head(3)\n",
    ")\n",
    "\n",
    "print(\"\\\\nPrime 5 categorie:\")\n",
    "print(\n",
    "    csv_data[\"categories\"][\n",
    "        [\"ProductCategoryID\", \"ParentProductCategoryID\", \"Name\"]\n",
    "    ].head()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dce9e305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Trovati 4 gruppi di documenti\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nDocumento: Vintage Trailblazer X-1 Mountain Bike (1995)\n",
      "  PDF: S√¨\n",
      "  Annotazioni: 3\n",
      "    - Vintage Trailblazer X-1 Mountain Bike (1995) Table 1.json\n",
      "    - Vintage Trailblazer X-1 Mountain Bike (1995).jpg\n",
      "    - Vintage Trailblazer X-1 Mountain Bike (1995) Table 2.json\n",
      "\\nDocumento: LL Mountain Handlebars (Black)\n",
      "  PDF: S√¨\n",
      "  Annotazioni: 2\n",
      "    - LL Mountain Handlebars (Black) Table 1.json\n",
      "    - LL Mountain Handlebars (Black) Fig 1.jpg\n",
      "\\nDocumento: Long-Sleeve Logo Jersey (M)\n",
      "  PDF: S√¨\n",
      "  Annotazioni: 2\n",
      "    - Long-Sleeve Logo Jersey (M) Table 1.json\n",
      "    - Long-Sleeve Logo Jersey (M) Fig 1.jpg\n",
      "\\nDocumento: Mountain Bike Manual\n",
      "  PDF: S√¨\n",
      "  Annotazioni: 2\n",
      "    - Mountain Bike Manual Table 1.json\n",
      "    - Mountain Bike Manual Table 2.json\n"
     ]
    }
   ],
   "source": [
    "# Analisi dei documenti e annotazioni in IngestedDocuments\n",
    "def analyze_ingested_documents():\n",
    "    \"\"\"Analizza la struttura dei documenti e delle loro annotazioni\"\"\"\n",
    "    docs_path = Path(\"../data/IngestedDocuments\")\n",
    "\n",
    "    # Raggruppa i file per base name (senza estensione e senza suffissi)\n",
    "    files = list(docs_path.glob(\"*\"))\n",
    "    documents = {}\n",
    "\n",
    "    for file in files:\n",
    "        # Estrae il nome del documento\n",
    "        name = file.name\n",
    "\n",
    "        # Identifica il tipo di file\n",
    "        if name.endswith(\".pdf\"):\n",
    "            base_name = name.replace(\".pdf\", \"\")\n",
    "            if base_name not in documents:\n",
    "                documents[base_name] = {\"pdf\": None, \"annotations\": []}\n",
    "            documents[base_name][\"pdf\"] = file\n",
    "\n",
    "        elif name.endswith(\".jpg\"):\n",
    "            # Rimuove \" Fig X\" dalla fine se presente\n",
    "            base_name = name.replace(\".jpg\", \"\")\n",
    "            if \" Fig \" in base_name:\n",
    "                base_name = base_name.split(\" Fig \")[0]\n",
    "            if base_name not in documents:\n",
    "                documents[base_name] = {\"pdf\": None, \"annotations\": []}\n",
    "            documents[base_name][\"annotations\"].append(file)\n",
    "\n",
    "        elif name.endswith(\".json\"):\n",
    "            # Rimuove \" Table X\" dalla fine se presente\n",
    "            base_name = name.replace(\".json\", \"\")\n",
    "            if \" Table \" in base_name:\n",
    "                base_name = base_name.split(\" Table \")[0]\n",
    "            if base_name not in documents:\n",
    "                documents[base_name] = {\"pdf\": None, \"annotations\": []}\n",
    "            documents[base_name][\"annotations\"].append(file)\n",
    "\n",
    "    logger.info(f\"Trovati {len(documents)} gruppi di documenti\")\n",
    "\n",
    "    for doc_name, doc_data in documents.items():\n",
    "        print(f\"\\\\nDocumento: {doc_name}\")\n",
    "        print(f\"  PDF: {'S√¨' if doc_data['pdf'] else 'Mancante'}\")\n",
    "        print(f\"  Annotazioni: {len(doc_data['annotations'])}\")\n",
    "        for ann in doc_data[\"annotations\"]:\n",
    "            print(f\"    - {ann.name}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Analizza i documenti\n",
    "document_structure = analyze_ingested_documents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2a21276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metodo create_product_nodes aggiunto alla classe\n"
     ]
    }
   ],
   "source": [
    "# Estensione della classe KnowledgeGraphBuilder con metodi per creare i nodi\n",
    "class KnowledgeGraphBuilder(KnowledgeGraphBuilder):\n",
    "    def create_product_nodes(self, products_df, categories_df, models_df):\n",
    "        \"\"\"Crea i nodi per i prodotti con le loro propriet√†\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            for _, product in products_df.iterrows():\n",
    "                try:\n",
    "                    # Trova la categoria (gestisce i valori NaN)\n",
    "                    category_name = \"Unknown\"\n",
    "                    if pd.notna(product.get(\"ProductCategoryID\")):\n",
    "                        category = categories_df[\n",
    "                            categories_df[\"ProductCategoryID\"]\n",
    "                            == product[\"ProductCategoryID\"]\n",
    "                        ]\n",
    "                        if not category.empty:\n",
    "                            category_name = str(category[\"Name\"].iloc[0])\n",
    "\n",
    "                    # Trova il modello (gestisce i valori NaN)\n",
    "                    model_name = \"Unknown\"\n",
    "                    if pd.notna(product.get(\"ProductModelID\")):\n",
    "                        model = models_df[\n",
    "                            models_df[\"ProductModelID\"] == product[\"ProductModelID\"]\n",
    "                        ]\n",
    "                        if not model.empty:\n",
    "                            model_name = str(model[\"Name\"].iloc[0])\n",
    "\n",
    "                    # Crea il nodo prodotto\n",
    "                    query = \"\"\"\n",
    "                    CREATE (p:Product {\n",
    "                        product_id: $product_id,\n",
    "                        name: $name,\n",
    "                        product_number: $product_number,\n",
    "                        color: $color,\n",
    "                        standard_cost: $standard_cost,\n",
    "                        list_price: $list_price,\n",
    "                        size: $size,\n",
    "                        weight: $weight,\n",
    "                        category_id: $category_id,\n",
    "                        category_name: $category_name,\n",
    "                        model_id: $model_id,\n",
    "                        model_name: $model_name,\n",
    "                        sell_start_date: $sell_start_date\n",
    "                    })\n",
    "                    \"\"\"\n",
    "\n",
    "                    # Prepara i parametri con gestione sicura dei tipi\n",
    "                    params = {\n",
    "                        \"product_id\": int(product[\"ProductID\"]),\n",
    "                        \"name\": str(product[\"Name\"]),\n",
    "                        \"product_number\": str(product[\"ProductNumber\"]),\n",
    "                        \"color\": str(\n",
    "                            product.get(\"Color\", \"\")\n",
    "                            if pd.notna(product.get(\"Color\"))\n",
    "                            else \"\"\n",
    "                        ),\n",
    "                        \"standard_cost\": float(\n",
    "                            str(product.get(\"StandardCost\", \"0\")).replace(\",\", \".\")\n",
    "                        )\n",
    "                        if pd.notna(product.get(\"StandardCost\"))\n",
    "                        else 0.0,\n",
    "                        \"list_price\": float(\n",
    "                            str(product.get(\"ListPrice\", \"0\")).replace(\",\", \".\")\n",
    "                        )\n",
    "                        if pd.notna(product.get(\"ListPrice\"))\n",
    "                        else 0.0,\n",
    "                        \"size\": str(\n",
    "                            product.get(\"Size\", \"\")\n",
    "                            if pd.notna(product.get(\"Size\"))\n",
    "                            else \"\"\n",
    "                        ),\n",
    "                        \"weight\": str(\n",
    "                            product.get(\"Weight\", \"\")\n",
    "                            if pd.notna(product.get(\"Weight\"))\n",
    "                            else \"\"\n",
    "                        ),\n",
    "                        \"category_id\": int(product[\"ProductCategoryID\"])\n",
    "                        if pd.notna(product.get(\"ProductCategoryID\"))\n",
    "                        else None,\n",
    "                        \"category_name\": category_name,\n",
    "                        \"model_id\": int(product[\"ProductModelID\"])\n",
    "                        if pd.notna(product.get(\"ProductModelID\"))\n",
    "                        else None,\n",
    "                        \"model_name\": model_name,\n",
    "                        \"sell_start_date\": str(\n",
    "                            product.get(\"SellStartDate\", \"\")\n",
    "                            if pd.notna(product.get(\"SellStartDate\"))\n",
    "                            else \"\"\n",
    "                        ),\n",
    "                    }\n",
    "\n",
    "                    session.run(query, **params)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(\n",
    "                        f\"Errore nel creare il nodo per il prodotto {product.get('ProductID', 'Unknown')}: {e}\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            logger.info(f\"Creati {len(products_df)} nodi Product\")\n",
    "\n",
    "\n",
    "print(\"Metodo create_product_nodes aggiunto alla classe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf8d18c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metodo create_document_nodes aggiunto alla classe\n"
     ]
    }
   ],
   "source": [
    "# Aggiunta di metodi per creare documenti e annotazioni\n",
    "class KnowledgeGraphBuilder(KnowledgeGraphBuilder):\n",
    "    def create_document_nodes(self, document_structure):\n",
    "        \"\"\"Crea i nodi per i documenti PDF e le loro annotazioni\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            for doc_name, doc_data in document_structure.items():\n",
    "                # Crea il nodo documento PDF se esiste\n",
    "                if doc_data[\"pdf\"]:\n",
    "                    pdf_file = doc_data[\"pdf\"]\n",
    "\n",
    "                    query = \"\"\"\n",
    "                    CREATE (d:Document {\n",
    "                        filename: $filename,\n",
    "                        document_name: $document_name,\n",
    "                        file_path: $file_path,\n",
    "                        file_type: 'PDF',\n",
    "                        file_size: $file_size\n",
    "                    })\n",
    "                    \"\"\"\n",
    "\n",
    "                    session.run(\n",
    "                        query,\n",
    "                        filename=pdf_file.name,\n",
    "                        document_name=doc_name,\n",
    "                        file_path=str(pdf_file),\n",
    "                        file_size=pdf_file.stat().st_size if pdf_file.exists() else 0,\n",
    "                    )\n",
    "\n",
    "                    # Crea i nodi annotazione e le relazioni\n",
    "                    for annotation_file in doc_data[\"annotations\"]:\n",
    "                        ann_type = (\n",
    "                            \"Image\" if annotation_file.suffix == \".jpg\" else \"Table\"\n",
    "                        )\n",
    "\n",
    "                        # Leggi il contenuto se √® un JSON\n",
    "                        content = None\n",
    "                        if annotation_file.suffix == \".json\":\n",
    "                            try:\n",
    "                                with open(annotation_file, \"r\") as f:\n",
    "                                    content = json.load(f)\n",
    "                            except Exception as e:\n",
    "                                logger.warning(\n",
    "                                    f\"Errore nel leggere {annotation_file}: {e}\"\n",
    "                                )\n",
    "\n",
    "                        # Crea il nodo annotazione\n",
    "                        ann_query = \"\"\"\n",
    "                        CREATE (a:Annotation {\n",
    "                            filename: $filename,\n",
    "                            annotation_type: $annotation_type,\n",
    "                            file_path: $file_path,\n",
    "                            content: $content,\n",
    "                            file_size: $file_size\n",
    "                        })\n",
    "                        \"\"\"\n",
    "\n",
    "                        session.run(\n",
    "                            ann_query,\n",
    "                            filename=annotation_file.name,\n",
    "                            annotation_type=ann_type,\n",
    "                            file_path=str(annotation_file),\n",
    "                            content=json.dumps(content) if content else None,\n",
    "                            file_size=annotation_file.stat().st_size\n",
    "                            if annotation_file.exists()\n",
    "                            else 0,\n",
    "                        )\n",
    "\n",
    "                        # Crea la relazione ANNOTATION tra documento e annotazione\n",
    "                        rel_query = \"\"\"\n",
    "                        MATCH (d:Document {filename: $doc_filename})\n",
    "                        MATCH (a:Annotation {filename: $ann_filename})\n",
    "                        CREATE (a)-[:ANNOTATION]->(d)\n",
    "                        \"\"\"\n",
    "\n",
    "                        session.run(\n",
    "                            rel_query,\n",
    "                            doc_filename=pdf_file.name,\n",
    "                            ann_filename=annotation_file.name,\n",
    "                        )\n",
    "\n",
    "            logger.info(\n",
    "                f\"Creati nodi per {len(document_structure)} documenti e relative annotazioni\"\n",
    "            )\n",
    "\n",
    "\n",
    "print(\"Metodo create_document_nodes aggiunto alla classe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23265891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metodo create_product_relationships aggiunto alla classe\n"
     ]
    }
   ],
   "source": [
    "# Aggiunta di metodi per creare relazioni tra prodotti\n",
    "class KnowledgeGraphBuilder(KnowledgeGraphBuilder):\n",
    "    def create_product_relationships(self):\n",
    "        \"\"\"Crea relazioni statiche/manuali tra prodotti\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # 1. Relazioni SAME_CATEGORY tra prodotti della stessa categoria\n",
    "            category_query = \"\"\"\n",
    "            MATCH (p1:Product), (p2:Product)\n",
    "            WHERE p1.category_id = p2.category_id \n",
    "            AND p1.product_id <> p2.product_id\n",
    "            AND p1.category_id IS NOT NULL\n",
    "            CREATE (p1)-[:SAME_CATEGORY]->(p2)\n",
    "            \"\"\"\n",
    "            result1 = session.run(category_query)\n",
    "            logger.info(\"Relazioni SAME_CATEGORY create\")\n",
    "\n",
    "            # 2. Relazioni SAME_MODEL tra prodotti dello stesso modello\n",
    "            model_query = \"\"\"\n",
    "            MATCH (p1:Product), (p2:Product)\n",
    "            WHERE p1.model_id = p2.model_id \n",
    "            AND p1.product_id <> p2.product_id\n",
    "            AND p1.model_id IS NOT NULL\n",
    "            CREATE (p1)-[:SAME_MODEL]->(p2)\n",
    "            \"\"\"\n",
    "            result2 = session.run(model_query)\n",
    "            logger.info(\"Relazioni SAME_MODEL create\")\n",
    "\n",
    "            # 3. Relazioni SIMILAR_PRICE tra prodotti con prezzi simili (¬±20%)\n",
    "            price_query = \"\"\"\n",
    "            MATCH (p1:Product), (p2:Product)\n",
    "            WHERE p1.product_id <> p2.product_id\n",
    "            AND p1.list_price > 0 AND p2.list_price > 0\n",
    "            AND abs(p1.list_price - p2.list_price) / p1.list_price <= 0.20\n",
    "            CREATE (p1)-[:SIMILAR_PRICE]->(p2)\n",
    "            \"\"\"\n",
    "            result3 = session.run(price_query)\n",
    "            logger.info(\"Relazioni SIMILAR_PRICE create\")\n",
    "\n",
    "            # 4. Relazioni manuali specifiche per prodotti molto correlati\n",
    "            # Esempio: tutti i frame road sono correlati\n",
    "            manual_relations = [\n",
    "                {\n",
    "                    \"filter1\": \"p1.name CONTAINS 'Road Frame'\",\n",
    "                    \"filter2\": \"p2.name CONTAINS 'Road Frame'\",\n",
    "                    \"relation\": \"COMPATIBLE_PRODUCT\",\n",
    "                },\n",
    "                {\n",
    "                    \"filter1\": \"p1.name CONTAINS 'Mountain'\",\n",
    "                    \"filter2\": \"p2.name CONTAINS 'Mountain'\",\n",
    "                    \"relation\": \"COMPATIBLE_PRODUCT\",\n",
    "                },\n",
    "                {\n",
    "                    \"filter1\": \"p1.name CONTAINS 'Helmet'\",\n",
    "                    \"filter2\": \"p2.name CONTAINS 'Jersey'\",\n",
    "                    \"relation\": \"COMPLEMENTARY_PRODUCT\",\n",
    "                },\n",
    "                {\n",
    "                    \"filter1\": \"p1.name CONTAINS 'Frame'\",\n",
    "                    \"filter2\": \"p2.name CONTAINS 'Handlebars'\",\n",
    "                    \"relation\": \"COMPLEMENTARY_PRODUCT\",\n",
    "                },\n",
    "            ]\n",
    "\n",
    "            for relation in manual_relations:\n",
    "                manual_query = f\"\"\"\n",
    "                MATCH (p1:Product), (p2:Product)\n",
    "                WHERE {relation[\"filter1\"]}\n",
    "                AND {relation[\"filter2\"]}\n",
    "                AND p1.product_id <> p2.product_id\n",
    "                CREATE (p1)-[:{relation[\"relation\"]}]->(p2)\n",
    "                \"\"\"\n",
    "                session.run(manual_query)\n",
    "                logger.info(f\"Relazioni {relation['relation']} create\")\n",
    "\n",
    "\n",
    "print(\"Metodo create_product_relationships aggiunto alla classe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a20f94d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metodi finali aggiunti alla classe KnowledgeGraphBuilder\n"
     ]
    }
   ],
   "source": [
    "# Metodi per collegare prodotti con documenti e query del grafo\n",
    "class KnowledgeGraphBuilder(KnowledgeGraphBuilder):\n",
    "    def create_product_document_relationships(self):\n",
    "        \"\"\"Crea relazioni tra prodotti e documenti basate sui nomi\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Cerca di collegare prodotti con documenti basandosi sui nomi\n",
    "            # Ad esempio, \"LL Mountain Handlebars\" dovrebbe collegarsi con il documento PDF\n",
    "\n",
    "            connect_query = \"\"\"\n",
    "            MATCH (p:Product), (d:Document)\n",
    "            WHERE d.document_name CONTAINS p.name \n",
    "            OR p.name CONTAINS d.document_name\n",
    "            OR (d.document_name CONTAINS 'Mountain' AND p.name CONTAINS 'Mountain')\n",
    "            OR (d.document_name CONTAINS 'Handlebars' AND p.name CONTAINS 'Handlebars')\n",
    "            OR (d.document_name CONTAINS 'Jersey' AND p.name CONTAINS 'Jersey')\n",
    "            CREATE (p)-[:DESCRIBED_BY]->(d)\n",
    "            \"\"\"\n",
    "\n",
    "            session.run(connect_query)\n",
    "            logger.info(\"Relazioni DESCRIBED_BY tra prodotti e documenti create\")\n",
    "\n",
    "    def get_graph_statistics(self):\n",
    "        \"\"\"Restituisce statistiche sul grafo creato\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            stats = {}\n",
    "\n",
    "            # Conta i nodi per tipo\n",
    "            node_counts = session.run(\"\"\"\n",
    "            MATCH (n)\n",
    "            RETURN labels(n)[0] as node_type, count(n) as count\n",
    "            ORDER BY count DESC\n",
    "            \"\"\")\n",
    "\n",
    "            stats[\"nodes\"] = {\n",
    "                record[\"node_type\"]: record[\"count\"] for record in node_counts\n",
    "            }\n",
    "\n",
    "            # Conta le relazioni per tipo\n",
    "            rel_counts = session.run(\"\"\"\n",
    "            MATCH ()-[r]->()\n",
    "            RETURN type(r) as relationship_type, count(r) as count\n",
    "            ORDER BY count DESC\n",
    "            \"\"\")\n",
    "\n",
    "            stats[\"relationships\"] = {\n",
    "                record[\"relationship_type\"]: record[\"count\"] for record in rel_counts\n",
    "            }\n",
    "\n",
    "            return stats\n",
    "\n",
    "    def query_similar_products(self, product_id, limit=5):\n",
    "        \"\"\"Trova prodotti simili a quello specificato\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            query = \"\"\"\n",
    "            MATCH (p:Product {product_id: $product_id})-[r]-(similar:Product)\n",
    "            RETURN similar.name as product_name, \n",
    "                   similar.product_id as product_id,\n",
    "                   type(r) as relationship_type,\n",
    "                   similar.list_price as price\n",
    "            ORDER BY similar.list_price\n",
    "            LIMIT $limit\n",
    "            \"\"\"\n",
    "\n",
    "            result = session.run(query, product_id=product_id, limit=limit)\n",
    "            return [dict(record) for record in result]\n",
    "\n",
    "\n",
    "print(\"Metodi finali aggiunti alla classe KnowledgeGraphBuilder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dd7e352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COSTRUZIONE DEL GRAFO DI CONOSCENZA ===\n",
      "\n",
      "üîÑ Questo script:\n",
      "   1. Legge i dati dai tuoi file CSV\n",
      "   2. Li carica in Neo4j come nodi del grafo\n",
      "   3. Crea relazioni intelligenti tra i nodi\n",
      "   4. Risultato: una knowledge base interrogabile!\n",
      "\n",
      "üåê URL Browser Neo4j: http://localhost:7474\n",
      "üîë Login: neo4j / password\n",
      "\n",
      "‚úÖ Neo4j is connected!\n",
      "1. Pulizia del database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Database pulito\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX product_id IF NOT EXISTS FOR (e:Product) ON (e.product_id)` has no effect.} {description: `RANGE INDEX product_id FOR (e:Product) ON (e.product_id)` already exists.} {position: None} for query: 'CREATE INDEX product_id IF NOT EXISTS FOR (p:Product) ON (p.product_id)'\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX product_name IF NOT EXISTS FOR (e:Product) ON (e.name)` has no effect.} {description: `RANGE INDEX product_name FOR (e:Product) ON (e.name)` already exists.} {position: None} for query: 'CREATE INDEX product_name IF NOT EXISTS FOR (p:Product) ON (p.name)'\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX document_filename IF NOT EXISTS FOR (e:Document) ON (e.filename)` has no effect.} {description: `RANGE INDEX document_filename FOR (e:Document) ON (e.filename)` already exists.} {position: None} for query: 'CREATE INDEX document_filename IF NOT EXISTS FOR (d:Document) ON (d.filename)'\n",
      "INFO:__main__:Indici creati\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX annotation_filename IF NOT EXISTS FOR (e:Annotation) ON (e.filename)` has no effect.} {description: `RANGE INDEX annotation_filename FOR (e:Annotation) ON (e.filename)` already exists.} {position: None} for query: 'CREATE INDEX annotation_filename IF NOT EXISTS FOR (a:Annotation) ON (a.filename)'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 680: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 706: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 712: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 713: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 714: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 715: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 716: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 717: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 718: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 719: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 720: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 721: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 722: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 723: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 724: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 725: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 726: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 727: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 728: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 729: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 730: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 731: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 732: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 733: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 734: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 735: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 736: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 737: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 738: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 739: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 740: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 741: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 742: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 743: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 744: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 745: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 746: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 747: 'Name'\n",
      "ERROR:__main__:Errore nel creare il nodo per il prodotto 748: 'Name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Creazione degli indici...\n",
      "3. Creazione dei nodi prodotto...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creati 100 nodi Product\n",
      "INFO:__main__:Creati nodi per 4 documenti e relative annotazioni\n",
      "INFO:__main__:Relazioni SAME_CATEGORY create\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (p2))} {position: line: 2, column: 13, offset: 13} for query: '\\n            MATCH (p1:Product), (p2:Product)\\n            WHERE p1.category_id = p2.category_id \\n            AND p1.product_id <> p2.product_id\\n            AND p1.category_id IS NOT NULL\\n            CREATE (p1)-[:SAME_CATEGORY]->(p2)\\n            '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Creazione dei nodi documento e annotazioni...\n",
      "5. Creazione delle relazioni tra prodotti...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Relazioni SAME_MODEL create\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (p2))} {position: line: 2, column: 13, offset: 13} for query: '\\n            MATCH (p1:Product), (p2:Product)\\n            WHERE p1.model_id = p2.model_id \\n            AND p1.product_id <> p2.product_id\\n            AND p1.model_id IS NOT NULL\\n            CREATE (p1)-[:SAME_MODEL]->(p2)\\n            '\n",
      "INFO:__main__:Relazioni SIMILAR_PRICE create\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (p2))} {position: line: 2, column: 13, offset: 13} for query: '\\n            MATCH (p1:Product), (p2:Product)\\n            WHERE p1.product_id <> p2.product_id\\n            AND p1.list_price > 0 AND p2.list_price > 0\\n            AND abs(p1.list_price - p2.list_price) / p1.list_price <= 0.20\\n            CREATE (p1)-[:SIMILAR_PRICE]->(p2)\\n            '\n",
      "INFO:__main__:Relazioni COMPATIBLE_PRODUCT create\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (p2))} {position: line: 2, column: 17, offset: 17} for query: \"\\n                MATCH (p1:Product), (p2:Product)\\n                WHERE p1.name CONTAINS 'Road Frame'\\n                AND p2.name CONTAINS 'Road Frame'\\n                AND p1.product_id <> p2.product_id\\n                CREATE (p1)-[:COMPATIBLE_PRODUCT]->(p2)\\n                \"\n",
      "INFO:__main__:Relazioni COMPATIBLE_PRODUCT create\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (p2))} {position: line: 2, column: 17, offset: 17} for query: \"\\n                MATCH (p1:Product), (p2:Product)\\n                WHERE p1.name CONTAINS 'Mountain'\\n                AND p2.name CONTAINS 'Mountain'\\n                AND p1.product_id <> p2.product_id\\n                CREATE (p1)-[:COMPATIBLE_PRODUCT]->(p2)\\n                \"\n",
      "INFO:__main__:Relazioni COMPLEMENTARY_PRODUCT create\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (p2))} {position: line: 2, column: 17, offset: 17} for query: \"\\n                MATCH (p1:Product), (p2:Product)\\n                WHERE p1.name CONTAINS 'Helmet'\\n                AND p2.name CONTAINS 'Jersey'\\n                AND p1.product_id <> p2.product_id\\n                CREATE (p1)-[:COMPLEMENTARY_PRODUCT]->(p2)\\n                \"\n",
      "INFO:__main__:Relazioni COMPLEMENTARY_PRODUCT create\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (p2))} {position: line: 2, column: 17, offset: 17} for query: \"\\n                MATCH (p1:Product), (p2:Product)\\n                WHERE p1.name CONTAINS 'Frame'\\n                AND p2.name CONTAINS 'Handlebars'\\n                AND p1.product_id <> p2.product_id\\n                CREATE (p1)-[:COMPLEMENTARY_PRODUCT]->(p2)\\n                \"\n",
      "INFO:__main__:Relazioni DESCRIBED_BY tra prodotti e documenti create\n",
      "INFO:neo4j.notifications:Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (d))} {position: line: 2, column: 13, offset: 13} for query: \"\\n            MATCH (p:Product), (d:Document)\\n            WHERE d.document_name CONTAINS p.name \\n            OR p.name CONTAINS d.document_name\\n            OR (d.document_name CONTAINS 'Mountain' AND p.name CONTAINS 'Mountain')\\n            OR (d.document_name CONTAINS 'Handlebars' AND p.name CONTAINS 'Handlebars')\\n            OR (d.document_name CONTAINS 'Jersey' AND p.name CONTAINS 'Jersey')\\n            CREATE (p)-[:DESCRIBED_BY]->(d)\\n            \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Collegamento prodotti con documenti...\n",
      "7. Statistiche del grafo creato:\n",
      "   Nodi: {'Product': 61, 'Annotation': 9, 'Document': 4}\n",
      "   Relazioni: {'SAME_CATEGORY': 1510, 'SIMILAR_PRICE': 572, 'COMPATIBLE_PRODUCT': 380, 'SAME_MODEL': 354, 'DESCRIBED_BY': 60, 'ANNOTATION': 9}\n",
      "\\n=== GRAFO COMPLETATO! ===\n",
      "Puoi ora esplorare il grafo usando Neo4j Browser su http://localhost:7474\n",
      "\n",
      "üîç Esempi di query Cypher per esplorare il grafo:\n",
      "MATCH (n) RETURN count(n) as total_nodes\n",
      "MATCH (p:Product) RETURN p.name, p.category_name LIMIT 10\n",
      "MATCH (d:Document)<-[:ANNOTATION]-(a:Annotation) RETURN d.document_name, count(a) as annotations\n"
     ]
    }
   ],
   "source": [
    "# COSTRUZIONE DEL GRAFO DI CONOSCENZA\n",
    "\n",
    "print(\"=== COSTRUZIONE DEL GRAFO DI CONOSCENZA ===\")\n",
    "print()\n",
    "print(\"üîÑ Questo script:\")\n",
    "print(\"   1. Legge i dati dai tuoi file CSV\")\n",
    "print(\"   2. Li carica in Neo4j come nodi del grafo\")\n",
    "print(\"   3. Crea relazioni intelligenti tra i nodi\")\n",
    "print(\"   4. Risultato: una knowledge base interrogabile!\")\n",
    "print()\n",
    "print(\"üåê URL Browser Neo4j: http://localhost:7474\")\n",
    "print(\"üîë Login: neo4j / password\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Test della connessione prima di procedere\n",
    "def test_neo4j_connection():\n",
    "    try:\n",
    "        test_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "        with test_driver.session() as session:\n",
    "            result = session.run(\"RETURN 'Neo4j is connected!' as message\")\n",
    "            message = result.single()[\"message\"]\n",
    "            test_driver.close()\n",
    "            return True, message\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "connected, message = test_neo4j_connection()\n",
    "\n",
    "if connected:\n",
    "    print(f\"‚úÖ {message}\")\n",
    "    build_graph = True  # Automaticamente procediamo se la connessione funziona\n",
    "else:\n",
    "    print(f\"‚ùå Errore di connessione: {message}\")\n",
    "    print()\n",
    "    print(\"üê≥ Per avviare Neo4j con Docker:\")\n",
    "    print(\n",
    "        \"docker run -p 7474:7474 -p 7687:7687 -d --env NEO4J_AUTH=neo4j/password neo4j:latest\"\n",
    "    )\n",
    "    print()\n",
    "    print(\"Poi riavvia questa cella!\")\n",
    "    build_graph = False\n",
    "\n",
    "if build_graph:\n",
    "    try:\n",
    "        # Inizializza il builder\n",
    "        kg_builder = KnowledgeGraphBuilder(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "\n",
    "        print(\"1. Pulizia del database...\")\n",
    "        kg_builder.clear_database()\n",
    "\n",
    "        print(\"2. Creazione degli indici...\")\n",
    "        kg_builder.create_indexes()\n",
    "\n",
    "        print(\"3. Creazione dei nodi prodotto...\")\n",
    "        kg_builder.create_product_nodes(\n",
    "            csv_data[\"products\"], csv_data[\"categories\"], csv_data[\"models\"]\n",
    "        )\n",
    "\n",
    "        print(\"4. Creazione dei nodi documento e annotazioni...\")\n",
    "        kg_builder.create_document_nodes(document_structure)\n",
    "\n",
    "        print(\"5. Creazione delle relazioni tra prodotti...\")\n",
    "        kg_builder.create_product_relationships()\n",
    "\n",
    "        print(\"6. Collegamento prodotti con documenti...\")\n",
    "        kg_builder.create_product_document_relationships()\n",
    "\n",
    "        print(\"7. Statistiche del grafo creato:\")\n",
    "        stats = kg_builder.get_graph_statistics()\n",
    "        print(f\"   Nodi: {stats['nodes']}\")\n",
    "        print(f\"   Relazioni: {stats['relationships']}\")\n",
    "\n",
    "        print(\"\\\\n=== GRAFO COMPLETATO! ===\")\n",
    "        print(\n",
    "            \"Puoi ora esplorare il grafo usando Neo4j Browser su http://localhost:7474\"\n",
    "        )\n",
    "        print()\n",
    "        print(\"üîç Esempi di query Cypher per esplorare il grafo:\")\n",
    "        print(\"MATCH (n) RETURN count(n) as total_nodes\")\n",
    "        print(\"MATCH (p:Product) RETURN p.name, p.category_name LIMIT 10\")\n",
    "        print(\n",
    "            \"MATCH (d:Document)<-[:ANNOTATION]-(a:Annotation) RETURN d.document_name, count(a) as annotations\"\n",
    "        )\n",
    "\n",
    "        kg_builder.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERRORE: {e}\")\n",
    "        print(\n",
    "            \"Verifica che Neo4j sia in esecuzione e che le credenziali siano corrette.\"\n",
    "        )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Neo4j non √® connesso.\")\n",
    "    print(\"üê≥ Per avviare Neo4j con Docker:\")\n",
    "    print(\n",
    "        \"docker run -p 7474:7474 -p 7687:7687 -d --env NEO4J_AUTH=neo4j/password neo4j:latest\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa33df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATCH (n)-[r:DESCRIBED_BY|ANNOTATION]-(m)\n",
    "# RETURN n, r, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f7c7074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ESEMPI DI QUERY SUL GRAFO ===\\n\n",
      "1. Prodotti simili al primo helmet:\n",
      "   Prodotto base: Sport-100 Helmet, Black (ID: 708)\n",
      "   ‚Üí Sport-100 Helmet, Red (SIMILAR_PRICE) - ‚Ç¨34.99\n",
      "   ‚Üí Sport-100 Helmet, Red (SIMILAR_PRICE) - ‚Ç¨34.99\n",
      "   ‚Üí Sport-100 Helmet, Blue (SIMILAR_PRICE) - ‚Ç¨34.99\n",
      "\\n2. Documenti e le loro annotazioni:\n",
      "   üìÑ Vintage Trailblazer X-1 Mountain Bike (1995): 3 annotazioni (Table, Image, Table)\n",
      "   üìÑ LL Mountain Handlebars (Black): 2 annotazioni (Table, Image)\n",
      "   üìÑ Long-Sleeve Logo Jersey (M): 2 annotazioni (Table, Image)\n",
      "   üìÑ Mountain Bike Manual: 2 annotazioni (Table, Table)\n",
      "\\n3. Prodotti per categoria:\n",
      "   üè∑Ô∏è Road Bikes: 35 prodotti\n",
      "   üè∑Ô∏è Mountain Bikes: 18 prodotti\n",
      "   üè∑Ô∏è Helmets: 3 prodotti\n",
      "   üè∑Ô∏è Forks: 3 prodotti\n",
      "   üè∑Ô∏è Socks: 2 prodotti\n",
      "\\n4. Relazioni prodotto-documento:\n",
      "   üì¶ Mountain-300 Black, 38 ‚Üê ‚Üí üìÑ Vintage Trailblazer X-1 Mountain Bike (1995)\n",
      "   üì¶ Mountain-200 Silver, 42 ‚Üê ‚Üí üìÑ Vintage Trailblazer X-1 Mountain Bike (1995)\n",
      "   üì¶ Mountain-300 Black, 48 ‚Üê ‚Üí üìÑ Vintage Trailblazer X-1 Mountain Bike (1995)\n",
      "   üì¶ Mountain-200 Silver, 38 ‚Üê ‚Üí üìÑ Vintage Trailblazer X-1 Mountain Bike (1995)\n",
      "   üì¶ Mountain-200 Black, 38 ‚Üê ‚Üí üìÑ Vintage Trailblazer X-1 Mountain Bike (1995)\n",
      "\\n=== QUERY CYPHER UTILI ===\n",
      "Puoi eseguire queste query nel Neo4j Browser (http://localhost:7474):\n",
      "\n",
      "# Visualizza tutto il grafo (attenzione con grafi grandi!):\n",
      "MATCH (n)-[r]-(m) RETURN n,r,m LIMIT 50\n",
      "\n",
      "# Trova prodotti di una categoria specifica:\n",
      "MATCH (p:Product) WHERE p.category_name = 'Road Bikes' RETURN p\n",
      "\n",
      "# Trova documenti con le loro annotazioni:\n",
      "MATCH (d:Document)<-[:ANNOTATION]-(a:Annotation) RETURN d,a\n",
      "\n",
      "# Trova percorsi tra due prodotti:\n",
      "MATCH path = (p1:Product)-[*1..3]-(p2:Product) WHERE p1.name CONTAINS 'Frame' AND p2.name CONTAINS 'Handlebars' RETURN path LIMIT 10\n"
     ]
    }
   ],
   "source": [
    "# ESEMPI DI QUERY SUL GRAFO\n",
    "# Questi esempi funzionano solo dopo aver costruito il grafo\n",
    "\n",
    "\n",
    "def demo_queries():\n",
    "    \"\"\"Esegue alcune query di esempio sul grafo\"\"\"\n",
    "    try:\n",
    "        kg_builder = KnowledgeGraphBuilder(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "\n",
    "        print(\"=== ESEMPI DI QUERY SUL GRAFO ===\\\\n\")\n",
    "\n",
    "        # 1. Trova un prodotto e i suoi simili\n",
    "        print(\"1. Prodotti simili al primo helmet:\")\n",
    "        with kg_builder.driver.session() as session:\n",
    "            # Trova il primo helmet\n",
    "            helmet_result = session.run(\"\"\"\n",
    "            MATCH (p:Product) \n",
    "            WHERE p.name CONTAINS 'Helmet' \n",
    "            RETURN p.product_id as id, p.name as name \n",
    "            LIMIT 1\n",
    "            \"\"\")\n",
    "\n",
    "            helmet = helmet_result.single()\n",
    "            if helmet:\n",
    "                print(f\"   Prodotto base: {helmet['name']} (ID: {helmet['id']})\")\n",
    "\n",
    "                # Trova prodotti simili\n",
    "                similar = kg_builder.query_similar_products(helmet[\"id\"], 3)\n",
    "                for product in similar:\n",
    "                    print(\n",
    "                        f\"   ‚Üí {product['product_name']} ({product['relationship_type']}) - ‚Ç¨{product['price']}\"\n",
    "                    )\n",
    "            else:\n",
    "                print(\"   Nessun helmet trovato\")\n",
    "\n",
    "        print(\"\\\\n2. Documenti e le loro annotazioni:\")\n",
    "        with kg_builder.driver.session() as session:\n",
    "            docs_result = session.run(\"\"\"\n",
    "            MATCH (d:Document)<-[:ANNOTATION]-(a:Annotation)\n",
    "            RETURN d.document_name as doc_name, \n",
    "                   collect(a.annotation_type) as annotation_types,\n",
    "                   count(a) as annotation_count\n",
    "            \"\"\")\n",
    "\n",
    "            for record in docs_result:\n",
    "                print(\n",
    "                    f\"   üìÑ {record['doc_name']}: {record['annotation_count']} annotazioni ({', '.join(record['annotation_types'])})\"\n",
    "                )\n",
    "\n",
    "        print(\"\\\\n3. Prodotti per categoria:\")\n",
    "        with kg_builder.driver.session() as session:\n",
    "            category_result = session.run(\"\"\"\n",
    "            MATCH (p:Product)\n",
    "            WHERE p.category_name IS NOT NULL\n",
    "            RETURN p.category_name as category, count(p) as product_count\n",
    "            ORDER BY product_count DESC\n",
    "            LIMIT 5\n",
    "            \"\"\")\n",
    "\n",
    "            for record in category_result:\n",
    "                print(f\"   üè∑Ô∏è {record['category']}: {record['product_count']} prodotti\")\n",
    "\n",
    "        print(\"\\\\n4. Relazioni prodotto-documento:\")\n",
    "        with kg_builder.driver.session() as session:\n",
    "            rel_result = session.run(\"\"\"\n",
    "            MATCH (p:Product)-[:DESCRIBED_BY]->(d:Document)\n",
    "            RETURN p.name as product_name, d.document_name as document_name\n",
    "            LIMIT 5\n",
    "            \"\"\")\n",
    "\n",
    "            for record in rel_result:\n",
    "                print(\n",
    "                    f\"   üì¶ {record['product_name']} ‚Üê ‚Üí üìÑ {record['document_name']}\"\n",
    "                )\n",
    "\n",
    "        kg_builder.close()\n",
    "\n",
    "        print(\"\\\\n=== QUERY CYPHER UTILI ===\")\n",
    "        print(\"Puoi eseguire queste query nel Neo4j Browser (http://localhost:7474):\")\n",
    "        print()\n",
    "        print(\"# Visualizza tutto il grafo (attenzione con grafi grandi!):\")\n",
    "        print(\"MATCH (n)-[r]-(m) RETURN n,r,m LIMIT 50\")\n",
    "        print()\n",
    "        print(\"# Trova prodotti di una categoria specifica:\")\n",
    "        print(\"MATCH (p:Product) WHERE p.category_name = 'Road Bikes' RETURN p\")\n",
    "        print()\n",
    "        print(\"# Trova documenti con le loro annotazioni:\")\n",
    "        print(\"MATCH (d:Document)<-[:ANNOTATION]-(a:Annotation) RETURN d,a\")\n",
    "        print()\n",
    "        print(\"# Trova percorsi tra due prodotti:\")\n",
    "        print(\n",
    "            \"MATCH path = (p1:Product)-[*1..3]-(p2:Product) WHERE p1.name CONTAINS 'Frame' AND p2.name CONTAINS 'Handlebars' RETURN path LIMIT 10\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERRORE nelle query: {e}\")\n",
    "        print(\n",
    "            \"Assicurati che il grafo sia stato costruito e che Neo4j sia in esecuzione.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Esegui le query demo solo se il grafo √® gi√† stato costruito\n",
    "try:\n",
    "    # Test rapido di connessione\n",
    "    test_kg = KnowledgeGraphBuilder(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "    with test_kg.driver.session() as session:\n",
    "        result = session.run(\"MATCH (n) RETURN count(n) as node_count\").single()\n",
    "        if result[\"node_count\"] > 0:\n",
    "            test_kg.close()\n",
    "            demo_queries()\n",
    "        else:\n",
    "            test_kg.close()\n",
    "            print(\n",
    "                \"Il grafo sembra vuoto. Costruisci prima il grafo eseguendo la cella precedente con build_graph = True\"\n",
    "            )\n",
    "except:\n",
    "    print(\"Neo4j non √® raggiungibile o il grafo non √® stato ancora costruito.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7856b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Dynamic Embedding Generator with Neo4j Integration created!\n"
     ]
    }
   ],
   "source": [
    "# EMBEDDINGS\n",
    "# This system automatically adapts to any table structure and generates embeddings\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class DynamicEmbeddingGenerator:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize with a sentence transformer model\"\"\"\n",
    "        print(f\"Loading embedding model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.embeddings_data = {\"embeddings\": [], \"metadata\": [], \"texts\": []}\n",
    "\n",
    "    def analyze_column_content(self, df, col):\n",
    "        \"\"\"Analyze column content to determine its characteristics\"\"\"\n",
    "        non_null_values = df[col].dropna()\n",
    "        if len(non_null_values) == 0:\n",
    "            return {\"type\": \"empty\"}\n",
    "\n",
    "        # Convert to strings for analysis\n",
    "        str_values = non_null_values.astype(str)\n",
    "        sample_values = str_values.head(10).tolist()\n",
    "\n",
    "        # Analyze patterns in the actual data\n",
    "        analysis = {\n",
    "            \"type\": \"data\",\n",
    "            \"unique_ratio\": len(non_null_values.unique()) / len(non_null_values),\n",
    "            \"avg_length\": str_values.str.len().mean(),\n",
    "            \"max_length\": str_values.str.len().max(),\n",
    "            \"has_numbers\": any(any(c.isdigit() for c in val) for val in sample_values),\n",
    "            \"all_numeric\": all(\n",
    "                self._is_numeric(val) for val in sample_values if val.strip()\n",
    "            ),\n",
    "            \"has_dates\": any(self._looks_like_date(val) for val in sample_values),\n",
    "            \"has_urls\": any(\n",
    "                \"http\" in val.lower() or \"www.\" in val.lower() for val in sample_values\n",
    "            ),\n",
    "            \"has_guids\": any(self._looks_like_guid(val) for val in sample_values),\n",
    "            \"is_boolean\": all(\n",
    "                val.lower() in [\"true\", \"false\", \"1\", \"0\", \"yes\", \"no\"]\n",
    "                for val in sample_values\n",
    "            ),\n",
    "            \"sample_values\": sample_values[:3],  # Keep a few examples\n",
    "        }\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def _is_numeric(self, val):\n",
    "        \"\"\"Check if a value is numeric\"\"\"\n",
    "        try:\n",
    "            float(str(val).replace(\",\", \"\"))\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _looks_like_date(self, val):\n",
    "        \"\"\"Check if a value looks like a date\"\"\"\n",
    "        val = str(val)\n",
    "        return (\n",
    "            len(val.split(\"-\")) == 3\n",
    "            or len(val.split(\"/\")) == 3\n",
    "            or len(val.split(\" \")) >= 2\n",
    "            and any(c.isdigit() for c in val)\n",
    "        )\n",
    "\n",
    "    def _looks_like_guid(self, val):\n",
    "        \"\"\"Check if a value looks like a GUID\"\"\"\n",
    "        val = str(val)\n",
    "        return len(val.replace(\"-\", \"\")) == 32 and all(\n",
    "            c in \"0123456789abcdefABCDEF-\" for c in val\n",
    "        )\n",
    "\n",
    "    def create_smart_text_representation(self, row, df, table_name=None):\n",
    "        \"\"\"Create text representation based on actual data patterns, not column names\"\"\"\n",
    "        text_parts = []\n",
    "\n",
    "        # Add table context\n",
    "        if table_name:\n",
    "            text_parts.append(f\"Table: {table_name}\")\n",
    "\n",
    "        # Analyze and categorize fields by semantic importance\n",
    "        high_importance = []  # Descriptive text, names, etc.\n",
    "        medium_importance = []  # Categories, types, etc.\n",
    "        low_importance = []  # IDs, technical fields, etc.\n",
    "\n",
    "        for col in df.columns:\n",
    "            value = row.get(col)\n",
    "            if pd.isna(value) or str(value).strip() == \"\":\n",
    "                continue\n",
    "\n",
    "            analysis = self.analyze_column_content(df, col)\n",
    "\n",
    "            # Skip empty or system fields\n",
    "            if analysis[\"type\"] == \"empty\" or analysis.get(\"has_guids\", False):\n",
    "                continue\n",
    "\n",
    "            str_value = str(value).strip()\n",
    "            field_text = f\"{col}: {str_value}\"\n",
    "\n",
    "            # Categorize by semantic value for embeddings\n",
    "            if (\n",
    "                analysis.get(\"avg_length\", 0) > 20\n",
    "                and analysis.get(\"unique_ratio\", 0) > 0.7\n",
    "            ):\n",
    "                # Long, unique text - likely descriptions, names\n",
    "                high_importance.append(field_text)\n",
    "            elif analysis.get(\"avg_length\", 0) > 5 and not analysis.get(\n",
    "                \"all_numeric\", False\n",
    "            ):\n",
    "                # Medium text, not purely numeric - likely categories, types\n",
    "                medium_importance.append(field_text)\n",
    "            elif analysis.get(\"all_numeric\", False) and not analysis.get(\n",
    "                \"has_dates\", False\n",
    "            ):\n",
    "                # Pure numbers - measurements, prices, quantities\n",
    "                low_importance.append(field_text)\n",
    "            elif analysis.get(\"unique_ratio\", 0) > 0.9:\n",
    "                # Highly unique - likely identifiers (but include for context)\n",
    "                low_importance.append(field_text)\n",
    "            else:\n",
    "                # Everything else\n",
    "                medium_importance.append(field_text)\n",
    "\n",
    "        # Combine in order of semantic importance\n",
    "        all_fields = high_importance + medium_importance + low_importance\n",
    "        text_parts.extend(all_fields)\n",
    "\n",
    "        return (\n",
    "            \". \".join(text_parts) if text_parts else f\"Table: {table_name}. Empty row.\"\n",
    "        )\n",
    "\n",
    "    def create_enhanced_text_representation(self, row, df, table_name=None):\n",
    "        \"\"\"Create enhanced text with additional context and relationships\"\"\"\n",
    "        # Get the basic smart representation\n",
    "        basic_text = self.create_smart_text_representation(row, df, table_name)\n",
    "\n",
    "        # Add statistical context about the data\n",
    "        enhanced_parts = [basic_text]\n",
    "\n",
    "        # Add table-level context\n",
    "        if df is not None:\n",
    "            total_rows = len(df)\n",
    "            total_cols = len(df.columns)\n",
    "            enhanced_parts.append(\n",
    "                f\"This is one of {total_rows} records with {total_cols} attributes\"\n",
    "            )\n",
    "\n",
    "        return \". \".join(enhanced_parts)\n",
    "\n",
    "    def process_csv_table(self, csv_path, related_data=None):\n",
    "        \"\"\"Process any CSV table dynamically\"\"\"\n",
    "        print(f\"Processing CSV: {csv_path}\")\n",
    "\n",
    "        # Load the CSV\n",
    "        df = pd.read_csv(csv_path, sep=\";\")\n",
    "        table_name = Path(csv_path).stem\n",
    "\n",
    "        # Analyze table structure (for metadata, not for hardcoded rules)\n",
    "        print(\n",
    "            f\"  Table: {table_name} with {len(df)} rows and {len(df.columns)} columns\"\n",
    "        )\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "        # Generate text representation for each row\n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                text = self.create_smart_text_representation(row, df, table_name)\n",
    "\n",
    "                # Create metadata\n",
    "                metadata = {\n",
    "                    \"id\": f\"{table_name}_{idx}\",\n",
    "                    \"type\": \"database_table\",\n",
    "                    \"source_table\": table_name,\n",
    "                    \"source_file\": str(csv_path),\n",
    "                    \"row_index\": idx,\n",
    "                    \"table_columns\": list(df.columns),\n",
    "                    \"total_rows\": len(df),\n",
    "                    \"total_columns\": len(df.columns),\n",
    "                }\n",
    "\n",
    "                # Try to find a primary identifier (any column with high uniqueness)\n",
    "                for col in df.columns:\n",
    "                    if pd.notna(row.get(col)):\n",
    "                        analysis = self.analyze_column_content(df, col)\n",
    "                        if (\n",
    "                            analysis.get(\"unique_ratio\", 0) > 0.9\n",
    "                        ):  # Highly unique = likely ID\n",
    "                            metadata[\"entity_id\"] = row[col]\n",
    "                            break\n",
    "\n",
    "                self.embeddings_data[\"texts\"].append(text)\n",
    "                self.embeddings_data[\"metadata\"].append(metadata)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing row {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"  Processed {len(df)} rows from {table_name}\")\n",
    "\n",
    "    def flatten_json_to_text(self, json_data, prefix=\"\"):\n",
    "        \"\"\"Recursively flatten JSON to natural language\"\"\"\n",
    "        text_parts = []\n",
    "\n",
    "        if isinstance(json_data, dict):\n",
    "            for key, value in json_data.items():\n",
    "                if isinstance(value, dict):\n",
    "                    # Recursive case\n",
    "                    nested_parts = self.flatten_json_to_text(value, f\"{prefix}{key}: \")\n",
    "                    text_parts.extend(nested_parts)\n",
    "                elif isinstance(value, list):\n",
    "                    # Handle lists\n",
    "                    list_items = [str(item) for item in value]\n",
    "                    text_parts.append(f\"{prefix}{key}: {', '.join(list_items)}\")\n",
    "                else:\n",
    "                    # Base case\n",
    "                    text_parts.append(f\"{prefix}{key}: {value}\")\n",
    "        elif isinstance(json_data, list):\n",
    "            for i, item in enumerate(json_data):\n",
    "                nested_parts = self.flatten_json_to_text(\n",
    "                    item, f\"{prefix}Item {i + 1}: \"\n",
    "                )\n",
    "                text_parts.extend(nested_parts)\n",
    "        else:\n",
    "            text_parts.append(f\"{prefix}{json_data}\")\n",
    "\n",
    "        return text_parts\n",
    "\n",
    "    def analyze_json_content(self, json_data):\n",
    "        \"\"\"Analyze JSON content to understand its structure and characteristics\"\"\"\n",
    "        analysis = {\n",
    "            'total_keys': 0,\n",
    "            'max_depth': 0,\n",
    "            'has_arrays': False,\n",
    "            'has_nested_objects': False,\n",
    "            'key_types': {},\n",
    "            'content_indicators': [],\n",
    "            'data_complexity': 'simple'\n",
    "        }\n",
    "        \n",
    "        def analyze_recursive(data, depth=0):\n",
    "            analysis['max_depth'] = max(analysis['max_depth'], depth)\n",
    "            \n",
    "            if isinstance(data, dict):\n",
    "                analysis['total_keys'] += len(data.keys())\n",
    "                for key, value in data.items():\n",
    "                    # Collect key names for content analysis (but don't hardcode meanings)\n",
    "                    key_lower = str(key).lower()\n",
    "                    if len(key_lower) > 2:  # Only meaningful keys\n",
    "                        analysis['content_indicators'].append(key_lower)\n",
    "                    \n",
    "                    if isinstance(value, dict):\n",
    "                        analysis['has_nested_objects'] = True\n",
    "                        analyze_recursive(value, depth + 1)\n",
    "                    elif isinstance(value, list):\n",
    "                        analysis['has_arrays'] = True\n",
    "                        if value:  # If list is not empty\n",
    "                            analyze_recursive(value[0], depth + 1)\n",
    "                    else:\n",
    "                        # Categorize value types\n",
    "                        value_type = type(value).__name__\n",
    "                        analysis['key_types'][value_type] = analysis['key_types'].get(value_type, 0) + 1\n",
    "            \n",
    "            elif isinstance(data, list):\n",
    "                analysis['has_arrays'] = True\n",
    "                if data:\n",
    "                    analyze_recursive(data[0], depth + 1)\n",
    "        \n",
    "        analyze_recursive(json_data)\n",
    "        \n",
    "        # Determine complexity based on structure\n",
    "        if analysis['max_depth'] > 3 or analysis['total_keys'] > 20:\n",
    "            analysis['data_complexity'] = 'complex'\n",
    "        elif analysis['max_depth'] > 1 or analysis['total_keys'] > 5:\n",
    "            analysis['data_complexity'] = 'moderate'\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def create_smart_json_text(self, json_data, filename, parent_document=None):\n",
    "        \"\"\"Create smart text representation of JSON data without hardcoded keywords\"\"\"\n",
    "        # Flatten JSON to text parts\n",
    "        text_parts = self.flatten_json_to_text(json_data)\n",
    "        \n",
    "        # Analyze the JSON structure\n",
    "        analysis = self.analyze_json_content(json_data)\n",
    "        \n",
    "        # Create document context\n",
    "        document_context = parent_document or filename\n",
    "        base_text = f\"Document: {document_context}. \"\n",
    "        \n",
    "        # Add structural information based on analysis\n",
    "        if analysis['data_complexity'] == 'complex':\n",
    "            base_text += f\"Complex structured data with {analysis['max_depth']} nested levels and {analysis['total_keys']} data fields. \"\n",
    "        elif analysis['data_complexity'] == 'moderate':\n",
    "            base_text += f\"Structured data with {analysis['total_keys']} fields. \"\n",
    "        \n",
    "        if analysis['has_arrays']:\n",
    "            base_text += \"Contains multiple data entries. \"\n",
    "        if analysis['has_nested_objects']:\n",
    "            base_text += \"Contains hierarchical information. \"\n",
    "        \n",
    "        # Add the actual content\n",
    "        content_text = \". \".join(text_parts)\n",
    "        full_text = base_text + content_text\n",
    "        \n",
    "        return full_text, analysis\n",
    "    \n",
    "    def process_json_table(self, json_path, parent_document=None):\n",
    "        \"\"\"Process JSON table files dynamically without hardcoded assumptions\"\"\"\n",
    "        print(f\"Processing JSON: {json_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            # Create smart text representation\n",
    "            filename = Path(json_path).stem\n",
    "            full_text, analysis = self.create_smart_json_text(json_data, filename, parent_document)\n",
    "            \n",
    "            metadata = {\n",
    "                \"id\": f\"json_{filename}\",\n",
    "                \"type\": \"json_table\",\n",
    "                \"source_file\": str(json_path),\n",
    "                \"parent_document\": parent_document,\n",
    "                \"json_keys\": list(json_data.keys()) if isinstance(json_data, dict) else [],\n",
    "                \"structure_analysis\": analysis,\n",
    "                \"content_preview\": str(json_data)[:200] + \"...\" if len(str(json_data)) > 200 else str(json_data)\n",
    "            }\n",
    "            \n",
    "            self.embeddings_data[\"texts\"].append(full_text)\n",
    "            self.embeddings_data[\"metadata\"].append(metadata)\n",
    "            \n",
    "            print(f\"  Processed JSON table: {filename}\")\n",
    "            print(f\"    Structure: {analysis['total_keys']} keys, {analysis['data_complexity']} complexity, depth {analysis['max_depth']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing JSON {json_path}: {e}\")\n",
    "\n",
    "    def process_all_data(self, data_dir=\"../data\"):\n",
    "        \"\"\"Process all data files in the directory\"\"\"\n",
    "        data_path = Path(data_dir)\n",
    "\n",
    "        print(\"üîÑ Starting dynamic embedding generation...\")\n",
    "\n",
    "        # 1. Process all CSV files\n",
    "        csv_files = list(data_path.glob(\"*.csv\"))\n",
    "\n",
    "        # Load related data for context (like categories for hierarchy)\n",
    "        related_data = {}\n",
    "        for csv_file in csv_files:\n",
    "            if \"category\" in csv_file.name.lower():\n",
    "                related_data[\"categories\"] = pd.read_csv(csv_file, sep=\";\")\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            self.process_csv_table(csv_file, related_data)\n",
    "\n",
    "        # 2. Process JSON tables from documents\n",
    "        json_files = list((data_path / \"IngestedDocuments\").glob(\"*.json\"))\n",
    "\n",
    "        for json_file in json_files:\n",
    "            # Extract parent document name\n",
    "            parent_doc = json_file.stem\n",
    "            if \" Table \" in parent_doc:\n",
    "                parent_doc = parent_doc.split(\" Table \")[0]\n",
    "\n",
    "            self.process_json_table(json_file, parent_doc)\n",
    "\n",
    "        print(f\"‚úÖ Processed {len(self.embeddings_data['texts'])} total text entries\")\n",
    "\n",
    "    def generate_embeddings(self):\n",
    "        \"\"\"Generate embeddings for all collected texts\"\"\"\n",
    "        if not self.embeddings_data[\"texts\"]:\n",
    "            print(\"No texts to embed!\")\n",
    "            return\n",
    "\n",
    "        print(\n",
    "            f\"Generating embeddings for {len(self.embeddings_data['texts'])} texts...\"\n",
    "        )\n",
    "\n",
    "        # Generate embeddings in batches for memory efficiency\n",
    "        batch_size = 32\n",
    "        all_embeddings = []\n",
    "\n",
    "        for i in range(0, len(self.embeddings_data[\"texts\"]), batch_size):\n",
    "            batch_texts = self.embeddings_data[\"texts\"][i : i + batch_size]\n",
    "            batch_embeddings = self.model.encode(batch_texts)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "            print(\n",
    "                f\"  Processed batch {i // batch_size + 1}/{(len(self.embeddings_data['texts']) - 1) // batch_size + 1}\"\n",
    "            )\n",
    "\n",
    "        self.embeddings_data[\"embeddings\"] = np.array(all_embeddings)\n",
    "        print(\n",
    "            f\"Generated embeddings shape: {self.embeddings_data['embeddings'].shape}\"\n",
    "        )\n",
    "\n",
    "    def save_embeddings(self, output_path=\"knowledge_graph_embeddings.pkl\"):\n",
    "        \"\"\"Save embeddings with metadata\"\"\"\n",
    "        print(f\"üíæ Saving embeddings to {output_path}\")\n",
    "\n",
    "        # Get model name safely\n",
    "        try:\n",
    "            model_name = getattr(self.model, 'model_name', 'unknown')\n",
    "            if model_name == 'unknown':\n",
    "                # Try alternative ways to get model name\n",
    "                if hasattr(self.model, '_modules') and '0' in self.model._modules:\n",
    "                    model_name = getattr(self.model._modules['0'], 'model_name', 'all-MiniLM-L6-v2')\n",
    "                else:\n",
    "                    model_name = 'all-MiniLM-L6-v2'  # Default fallback\n",
    "        except:\n",
    "            model_name = 'all-MiniLM-L6-v2'\n",
    "\n",
    "        # Add generation metadata\n",
    "        self.embeddings_data[\"generation_info\"] = {\n",
    "            \"model_name\": model_name,\n",
    "            \"total_entries\": len(self.embeddings_data[\"texts\"]),\n",
    "            \"embedding_dimension\": self.embeddings_data[\"embeddings\"].shape[1]\n",
    "            if len(self.embeddings_data[\"embeddings\"]) > 0\n",
    "            else 0,\n",
    "            \"generation_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "        }\n",
    "\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            pickle.dump(self.embeddings_data, f)\n",
    "\n",
    "        print(\n",
    "            f\"Saved {len(self.embeddings_data['texts'])} embeddings to {output_path}\"\n",
    "        )\n",
    "\n",
    "        # Print summary\n",
    "        print(\"EMBEDDING SUMMARY:\")\n",
    "        print(f\"   Total embeddings: {len(self.embeddings_data['texts'])}\")\n",
    "        print(f\"   Embedding dimension: {self.embeddings_data['embeddings'].shape[1]}\")\n",
    "\n",
    "        # Count by type\n",
    "        type_counts = {}\n",
    "        for metadata in self.embeddings_data[\"metadata\"]:\n",
    "            type_key = metadata[\"type\"]\n",
    "            type_counts[type_key] = type_counts.get(type_key, 0) + 1\n",
    "\n",
    "        for type_name, count in type_counts.items():\n",
    "            print(f\"   {type_name}: {count}\")\n",
    "\n",
    "    def load_embeddings(self, input_path=\"knowledge_graph_embeddings.pkl\"):\n",
    "        \"\"\"Load previously saved embeddings\"\"\"\n",
    "        print(f\"üìÇ Loading embeddings from {input_path}\")\n",
    "\n",
    "        with open(input_path, \"rb\") as f:\n",
    "            self.embeddings_data = pickle.load(f)\n",
    "\n",
    "        print(f\"‚úÖ Loaded {len(self.embeddings_data['texts'])} embeddings\")\n",
    "        return self.embeddings_data\n",
    "    \n",
    "    # üéØ SIMPLE ALTERNATIVE METHODS (if you prefer np.save approach)\n",
    "    def save_embeddings_simple(self, base_path=\"embeddings\"):\n",
    "        \"\"\"Save embeddings using simple np.save approach\"\"\"\n",
    "        if not self.embeddings_data[\"embeddings\"]:\n",
    "            print(\"No embeddings to save!\")\n",
    "            return\n",
    "            \n",
    "        print(f\"üíæ Saving embeddings using simple np.save approach...\")\n",
    "        \n",
    "        # Save just the embeddings matrix\n",
    "        np.save(f\"{base_path}.npy\", self.embeddings_data[\"embeddings\"])\n",
    "        \n",
    "        # Save texts and metadata separately as JSON for human readability\n",
    "        import json\n",
    "        simple_data = {\n",
    "            \"texts\": self.embeddings_data[\"texts\"],\n",
    "            \"metadata\": self.embeddings_data[\"metadata\"]\n",
    "        }\n",
    "        \n",
    "        with open(f\"{base_path}_metadata.json\", \"w\") as f:\n",
    "            json.dump(simple_data, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Saved:\")\n",
    "        print(f\"   - {base_path}.npy (embeddings matrix)\")\n",
    "        print(f\"   - {base_path}_metadata.json (texts and metadata)\")\n",
    "    \n",
    "    def load_embeddings_simple(self, base_path=\"embeddings\"):\n",
    "        \"\"\"Load embeddings using simple np.load approach\"\"\"\n",
    "        import json\n",
    "        \n",
    "        print(f\"üìÇ Loading embeddings using simple np.load approach...\")\n",
    "        \n",
    "        # Load embeddings matrix\n",
    "        embeddings = np.load(f\"{base_path}.npy\")\n",
    "        \n",
    "        # Load texts and metadata\n",
    "        with open(f\"{base_path}_metadata.json\", \"r\") as f:\n",
    "            simple_data = json.load(f)\n",
    "        \n",
    "        self.embeddings_data = {\n",
    "            \"embeddings\": embeddings,\n",
    "            \"texts\": simple_data[\"texts\"],\n",
    "            \"metadata\": simple_data[\"metadata\"]\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(self.embeddings_data['texts'])} embeddings\")\n",
    "        return self.embeddings_data\n",
    "    \n",
    "    # üîó NEO4J INTEGRATION METHODS\n",
    "    def create_embedding_index_mapping(self):\n",
    "        \"\"\"Create a mapping from metadata IDs to embedding indices\"\"\"\n",
    "        embedding_index = {}\n",
    "        for idx, metadata in enumerate(self.embeddings_data[\"metadata\"]):\n",
    "            # Create multiple ways to find embeddings\n",
    "            embedding_index[metadata[\"id\"]] = idx\n",
    "            \n",
    "            # For database tables, also index by entity_id if available\n",
    "            if metadata[\"type\"] == \"database_table\" and \"entity_id\" in metadata:\n",
    "                entity_key = f\"{metadata['source_table']}_{metadata['entity_id']}\"\n",
    "                embedding_index[entity_key] = idx\n",
    "            \n",
    "            # For JSON tables, index by source file\n",
    "            if metadata[\"type\"] == \"json_table\":\n",
    "                embedding_index[metadata[\"source_file\"]] = idx\n",
    "        \n",
    "        return embedding_index\n",
    "    \n",
    "    def get_embedding_by_id(self, item_id):\n",
    "        \"\"\"Get embedding and metadata for a specific item\"\"\"\n",
    "        embedding_index = self.create_embedding_index_mapping()\n",
    "        \n",
    "        if item_id in embedding_index:\n",
    "            idx = embedding_index[item_id]\n",
    "            return {\n",
    "                \"embedding\": self.embeddings_data[\"embeddings\"][idx],\n",
    "                \"text\": self.embeddings_data[\"texts\"][idx],\n",
    "                \"metadata\": self.embeddings_data[\"metadata\"][idx],\n",
    "                \"index\": idx\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    def save_embeddings_with_neo4j_integration(self, output_path=\"knowledge_graph_embeddings.pkl\", neo4j_builder=None):\n",
    "        \"\"\"Save embeddings and update Neo4j nodes with embedding references\"\"\"\n",
    "        # Save embeddings normally first\n",
    "        self.save_embeddings(output_path)\n",
    "        \n",
    "        if neo4j_builder is None:\n",
    "            print(\"‚ö†Ô∏è No Neo4j builder provided - skipping graph integration\")\n",
    "            return\n",
    "        \n",
    "        print(\"üîó Integrating embeddings with Neo4j knowledge graph...\")\n",
    "        \n",
    "        # Create embedding index\n",
    "        embedding_index = self.create_embedding_index_mapping()\n",
    "        \n",
    "        with neo4j_builder.driver.session() as session:\n",
    "            # Update Product nodes with embedding references\n",
    "            for idx, metadata in enumerate(self.embeddings_data[\"metadata\"]):\n",
    "                if metadata[\"type\"] == \"database_table\":\n",
    "                    source_table = metadata[\"source_table\"]\n",
    "                    \n",
    "                    if source_table == \"Product\" and \"entity_id\" in metadata:\n",
    "                        # Update Product nodes\n",
    "                        query = \"\"\"\n",
    "                        MATCH (p:Product {product_id: $product_id})\n",
    "                        SET p.embedding_id = $embedding_id,\n",
    "                            p.embedding_index = $embedding_index,\n",
    "                            p.embedding_file = $embedding_file,\n",
    "                            p.has_embedding = true\n",
    "                        \"\"\"\n",
    "                        session.run(query, \n",
    "                                   product_id=int(metadata[\"entity_id\"]),\n",
    "                                   embedding_id=metadata[\"id\"],\n",
    "                                   embedding_index=idx,\n",
    "                                   embedding_file=output_path)\n",
    "                    \n",
    "                    elif source_table == \"ProductCategory\" and \"entity_id\" in metadata:\n",
    "                        # Update Category nodes if they exist\n",
    "                        query = \"\"\"\n",
    "                        MATCH (c:Category {category_id: $category_id})\n",
    "                        SET c.embedding_id = $embedding_id,\n",
    "                            c.embedding_index = $embedding_index,\n",
    "                            c.embedding_file = $embedding_file,\n",
    "                            c.has_embedding = true\n",
    "                        \"\"\"\n",
    "                        try:\n",
    "                            session.run(query,\n",
    "                                       category_id=int(metadata[\"entity_id\"]),\n",
    "                                       embedding_id=metadata[\"id\"],\n",
    "                                       embedding_index=idx,\n",
    "                                       embedding_file=output_path)\n",
    "                        except:\n",
    "                            pass  # Category nodes might not exist\n",
    "                \n",
    "                elif metadata[\"type\"] == \"json_table\":\n",
    "                    # Update Document/Annotation nodes\n",
    "                    parent_doc = metadata.get(\"parent_document\", \"\")\n",
    "                    if parent_doc:\n",
    "                        query = \"\"\"\n",
    "                        MATCH (d:Document) \n",
    "                        WHERE d.document_name CONTAINS $parent_doc\n",
    "                        SET d.embedding_id = $embedding_id,\n",
    "                            d.embedding_index = $embedding_index,\n",
    "                            d.embedding_file = $embedding_file,\n",
    "                            d.has_embedding = true\n",
    "                        \"\"\"\n",
    "                        session.run(query,\n",
    "                                   parent_doc=parent_doc,\n",
    "                                   embedding_id=metadata[\"id\"],\n",
    "                                   embedding_index=idx,\n",
    "                                   embedding_file=output_path)\n",
    "        \n",
    "        print(f\"‚úÖ Updated Neo4j nodes with embedding references\")\n",
    "        print(f\"   - Embedding file: {output_path}\")\n",
    "        print(f\"   - Total embeddings: {len(self.embeddings_data['embeddings'])}\")\n",
    "\n",
    "\n",
    "# üîó NEO4J + EMBEDDINGS HYBRID SEARCH CLASS\n",
    "class HybridSearchEngine:\n",
    "    def __init__(self, neo4j_builder, embedding_generator):\n",
    "        \"\"\"Initialize hybrid search with both Neo4j and embeddings\"\"\"\n",
    "        self.neo4j = neo4j_builder\n",
    "        self.embeddings = embedding_generator\n",
    "        self.embedding_index = embedding_generator.create_embedding_index_mapping()\n",
    "    \n",
    "    def semantic_graph_search(self, query, top_k=5, graph_filter=None):\n",
    "        \"\"\"Search using embeddings + filter by graph properties\"\"\"\n",
    "        print(f\"üîç Hybrid search for: '{query}'\")\n",
    "        \n",
    "        # 1. Get semantic matches from embeddings\n",
    "        query_embedding = self.embeddings.model.encode([query])\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings.embeddings_data[\"embeddings\"])[0]\n",
    "        \n",
    "        # Get top candidates (more than needed)\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k * 3]\n",
    "        \n",
    "        results = []\n",
    "        with self.neo4j.driver.session() as session:\n",
    "            for idx in top_indices:\n",
    "                metadata = self.embeddings.embeddings_data[\"metadata\"][idx]\n",
    "                \n",
    "                # 2. Check if this item exists in Neo4j and matches graph filter\n",
    "                if metadata[\"type\"] == \"database_table\" and metadata[\"source_table\"] == \"Product\":\n",
    "                    if \"entity_id\" in metadata:\n",
    "                        # Query Neo4j for additional context\n",
    "                        graph_query = \"\"\"\n",
    "                        MATCH (p:Product {product_id: $product_id})\n",
    "                        OPTIONAL MATCH (p)-[r]-(related)\n",
    "                        RETURN p, collect(DISTINCT type(r)) as relationships, \n",
    "                               collect(DISTINCT labels(related)) as related_types\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        graph_result = session.run(graph_query, product_id=int(metadata[\"entity_id\"]))\n",
    "                        graph_data = graph_result.single()\n",
    "                        \n",
    "                        if graph_data and graph_data[\"p\"]:\n",
    "                            # Apply graph filter if provided\n",
    "                            if graph_filter is None or self._matches_graph_filter(graph_data, graph_filter):\n",
    "                                result = {\n",
    "                                    \"similarity\": similarities[idx],\n",
    "                                    \"text\": self.embeddings.embeddings_data[\"texts\"][idx],\n",
    "                                    \"metadata\": metadata,\n",
    "                                    \"graph_data\": dict(graph_data[\"p\"]),\n",
    "                                    \"relationships\": graph_data[\"relationships\"],\n",
    "                                    \"related_types\": graph_data[\"related_types\"]\n",
    "                                }\n",
    "                                results.append(result)\n",
    "                                \n",
    "                                if len(results) >= top_k:\n",
    "                                    break\n",
    "                else:\n",
    "                    # Non-product items (documents, etc.)\n",
    "                    result = {\n",
    "                        \"similarity\": similarities[idx],\n",
    "                        \"text\": self.embeddings.embeddings_data[\"texts\"][idx],\n",
    "                        \"metadata\": metadata,\n",
    "                        \"graph_data\": None\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    if len(results) >= top_k:\n",
    "                        break\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _matches_graph_filter(self, graph_data, graph_filter):\n",
    "        \"\"\"Check if graph data matches the filter criteria\"\"\"\n",
    "        # Example filters: {\"category\": \"Bikes\", \"price_range\": [100, 500]}\n",
    "        product = graph_data[\"p\"]\n",
    "        \n",
    "        if \"category\" in graph_filter:\n",
    "            if product.get(\"category_name\") != graph_filter[\"category\"]:\n",
    "                return False\n",
    "        \n",
    "        if \"price_range\" in graph_filter:\n",
    "            price = product.get(\"list_price\", 0)\n",
    "            min_price, max_price = graph_filter[\"price_range\"]\n",
    "            if not (min_price <= price <= max_price):\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def find_similar_products_in_category(self, product_id, category=None, top_k=5):\n",
    "        \"\"\"Find products similar to given product, optionally within a category\"\"\"\n",
    "        print(f\"üîó Finding similar products to {product_id}\")\n",
    "        \n",
    "        # Get the embedding for the source product\n",
    "        entity_key = f\"Product_{product_id}\"\n",
    "        source_embedding_data = self.embeddings.get_embedding_by_id(entity_key)\n",
    "        \n",
    "        if not source_embedding_data:\n",
    "            print(f\"‚ùå No embedding found for product {product_id}\")\n",
    "            return []\n",
    "        \n",
    "        # Calculate similarities\n",
    "        source_embedding = source_embedding_data[\"embedding\"].reshape(1, -1)\n",
    "        similarities = cosine_similarity(source_embedding, self.embeddings.embeddings_data[\"embeddings\"])[0]\n",
    "        \n",
    "        # Get candidates\n",
    "        top_indices = np.argsort(similarities)[::-1][1:top_k*2]  # Skip self\n",
    "        \n",
    "        results = []\n",
    "        with self.neo4j.driver.session() as session:\n",
    "            for idx in top_indices:\n",
    "                metadata = self.embeddings.embeddings_data[\"metadata\"][idx]\n",
    "                \n",
    "                if (metadata[\"type\"] == \"database_table\" and \n",
    "                    metadata[\"source_table\"] == \"Product\" and \n",
    "                    \"entity_id\" in metadata):\n",
    "                    \n",
    "                    # Get graph context\n",
    "                    graph_query = \"\"\"\n",
    "                    MATCH (p:Product {product_id: $product_id})\n",
    "                    RETURN p\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    graph_result = session.run(graph_query, product_id=int(metadata[\"entity_id\"]))\n",
    "                    graph_data = graph_result.single()\n",
    "                    \n",
    "                    if graph_data and graph_data[\"p\"]:\n",
    "                        product = dict(graph_data[\"p\"])\n",
    "                        \n",
    "                        # Apply category filter if specified\n",
    "                        if category is None or product.get(\"category_name\") == category:\n",
    "                            result = {\n",
    "                                \"similarity\": similarities[idx],\n",
    "                                \"product_id\": product[\"product_id\"],\n",
    "                                \"name\": product[\"name\"],\n",
    "                                \"category\": product.get(\"category_name\", \"Unknown\"),\n",
    "                                \"price\": product.get(\"list_price\", 0),\n",
    "                                \"text\": self.embeddings.embeddings_data[\"texts\"][idx]\n",
    "                            }\n",
    "                            results.append(result)\n",
    "                            \n",
    "                            if len(results) >= top_k:\n",
    "                                break\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "print(\"üéØ Dynamic Embedding Generator with Neo4j Integration created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e45d9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DYNAMIC EMBEDDING GENERATION ===\n",
      "\n",
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "üîÑ Starting dynamic embedding generation...\n",
      "Processing CSV: ../data/Product.csv\n",
      "  Table: Product with 100 rows and 17 columns\n",
      "  Columns: ['ProductID', 'Name', 'ProductNumber', 'Color', 'StandardCost', 'ListPrice', 'Size', 'Weight', 'ProductCategoryID', 'ProductModelID', 'SellStartDate', 'SellEndDate', 'DiscontinuedDate', 'ThumbNailPhoto', 'ThumbnailPhotoFileName', 'rowguid', 'ModifiedDate']\n",
      "  Processed 100 rows from Product\n",
      "Processing CSV: ../data/SalesOrderHeader.csv\n",
      "  Table: SalesOrderHeader with 32 rows and 22 columns\n",
      "  Columns: ['SalesOrderID', 'RevisionNumber', 'OrderDate', 'DueDate', 'ShipDate', 'Status', 'OnlineOrderFlag', 'SalesOrderNumber', 'PurchaseOrderNumber', 'AccountNumber', 'CustomerID', 'ShipToAddressID', 'BillToAddressID', 'ShipMethod', 'CreditCardApprovalCode', 'SubTotal', 'TaxAmt', 'Freight', 'TotalDue', 'Comment', 'rowguid', 'ModifiedDate']\n",
      "  Processed 32 rows from SalesOrderHeader\n",
      "Processing CSV: ../data/ProductDescription.csv\n",
      "  Table: ProductDescription with 100 rows and 4 columns\n",
      "  Columns: ['ProductDescriptionID', 'Description', 'rowguid', 'ModifiedDate']\n",
      "  Processed 100 rows from ProductDescription\n",
      "Processing CSV: ../data/ProductCategory.csv\n",
      "  Table: ProductCategory with 41 rows and 5 columns\n",
      "  Columns: ['ProductCategoryID', 'ParentProductCategoryID', 'Name', 'rowguid', 'ModifiedDate']\n",
      "  Processed 41 rows from ProductCategory\n",
      "Processing CSV: ../data/SalesOrderDetail.csv\n",
      "  Table: SalesOrderDetail with 100 rows and 9 columns\n",
      "  Columns: ['SalesOrderID', 'SalesOrderDetailID', 'OrderQty', 'ProductID', 'UnitPrice', 'UnitPriceDiscount', 'LineTotal', 'rowguid', 'ModifiedDate']\n",
      "  Processed 100 rows from SalesOrderDetail\n",
      "Processing CSV: ../data/ProductModelProductDescription.csv\n",
      "  Table: ProductModelProductDescription with 100 rows and 5 columns\n",
      "  Columns: ['ProductModelID', 'ProductDescriptionID', 'Culture', 'rowguid', 'ModifiedDate']\n",
      "  Processed 100 rows from ProductModelProductDescription\n",
      "Processing CSV: ../data/ProductModel.csv\n",
      "  Table: ProductModel with 100 rows and 5 columns\n",
      "  Columns: ['ProductModelID', 'ProductDescriptionID', 'Culture', 'rowguid', 'ModifiedDate']\n",
      "  Processed 100 rows from ProductModel\n",
      "Processing JSON: ../data/IngestedDocuments/Vintage Trailblazer X-1 Mountain Bike (1995) Table 1.json\n",
      "  Processed JSON table: Vintage Trailblazer X-1 Mountain Bike (1995) Table 1\n",
      "    Structure: 7 keys, moderate complexity, depth 1\n",
      "Processing JSON: ../data/IngestedDocuments/LL Mountain Handlebars (Black) Table 1.json\n",
      "  Processed JSON table: LL Mountain Handlebars (Black) Table 1\n",
      "    Structure: 4 keys, simple complexity, depth 0\n",
      "Processing JSON: ../data/IngestedDocuments/Long-Sleeve Logo Jersey (M) Table 1.json\n",
      "  Processed JSON table: Long-Sleeve Logo Jersey (M) Table 1\n",
      "    Structure: 5 keys, simple complexity, depth 0\n",
      "Processing JSON: ../data/IngestedDocuments/Vintage Trailblazer X-1 Mountain Bike (1995) Table 2.json\n",
      "  Processed JSON table: Vintage Trailblazer X-1 Mountain Bike (1995) Table 2\n",
      "    Structure: 4 keys, simple complexity, depth 1\n",
      "Processing JSON: ../data/IngestedDocuments/Mountain Bike Manual Table 1.json\n",
      "  Processed JSON table: Mountain Bike Manual Table 1\n",
      "    Structure: 4 keys, simple complexity, depth 1\n",
      "Processing JSON: ../data/IngestedDocuments/Mountain Bike Manual Table 2.json\n",
      "  Processed JSON table: Mountain Bike Manual Table 2\n",
      "    Structure: 4 keys, simple complexity, depth 1\n",
      "‚úÖ Processed 579 total text entries\n",
      "\n",
      "==================================================\n",
      "üìã PREVIEW OF GENERATED TEXTS:\n",
      "==================================================\n",
      "\n",
      "üî∏ Example 1 (database_table):\n",
      "   Source: Product\n",
      "   Text: Table: Product. Name: HL Road Frame - Black, 58. ProductNumber: FR-R92B-58. Color: Black. Size: 58. SellStartDate: 2002-06-01 00:00:00.000. ThumbNailP...\n",
      "\n",
      "üî∏ Example 2 (database_table):\n",
      "   Source: Product\n",
      "   Text: Table: Product. Name: HL Road Frame - Red, 58. ProductNumber: FR-R92R-58. Color: Red. Size: 58. SellStartDate: 2002-06-01 00:00:00.000. ThumbNailPhoto...\n",
      "\n",
      "üî∏ Example 3 (database_table):\n",
      "   Source: Product\n",
      "   Text: Table: Product. Name: Sport-100 Helmet, Red. ProductNumber: HL-U509-R. Color: Red. SellStartDate: 2005-07-01 00:00:00.000. ThumbNailPhoto: 0x474946383...\n",
      "\n",
      "üî∏ Example 4 (database_table):\n",
      "   Source: Product\n",
      "   Text: Table: Product. Name: Sport-100 Helmet, Black. ProductNumber: HL-U509. Color: Black. SellStartDate: 2005-07-01 00:00:00.000. ThumbNailPhoto: 0x4749463...\n",
      "\n",
      "üî∏ Example 5 (database_table):\n",
      "   Source: Product\n",
      "   Text: Table: Product. Name: Mountain Bike Socks, M. ProductNumber: SO-B909-M. Color: White. Size: M. SellStartDate: 2005-07-01 00:00:00.000. SellEndDate: 20...\n",
      "Generating embeddings for 579 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d7d0c777dd4502b69f7c5a0bd4a877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 1/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe5e471c327494eaf7bb3bf78280202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 2/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a28c3e3657940aa9e756d3df24b1a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 3/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40c609265884f6bbbf28cdc467c83f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 4/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090848bcf1f5480582ca561eb03c1555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 5/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e30ea3f467e4afebb98882831198a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 6/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa81d4feeeeb4b90a934f6a8ca48fbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 7/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1621bbfecfb7483f8c6e9b629923c342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 8/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18edd1b31bd447d89d1631fdcf4b3bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 9/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5c9759fc324a1483e0f553e36ed5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 10/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e446a8d46e674a85b189627ebcb5fff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 11/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854b6b5ccc53499787b2e1276e02263b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 12/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691b3bdcc46f4fdeb2a7f5e4ac95cb95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 13/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e889029960fc498fa23a66f687358ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 14/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a822f5a24ab4b1686c6a03658850d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 15/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa9353195a343638e592ddce38af532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 16/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbfb43ba8cb46d9a32665db25eaec42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 17/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6852ad34d084f52981609ab1e1954cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 18/19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462eb8ed3fbc42c4bec98721818d7205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed batch 19/19\n",
      "Generated embeddings shape: (579, 384)\n",
      "‚ö†Ô∏è Neo4j integration failed: No module named 'notebooks'\n",
      "Saving embeddings without Neo4j integration...\n",
      "üíæ Saving embeddings to ../data/knowledge_graph_embeddings.pkl\n",
      "Saved 579 embeddings to ../data/knowledge_graph_embeddings.pkl\n",
      "EMBEDDING SUMMARY:\n",
      "   Total embeddings: 579\n",
      "   Embedding dimension: 384\n",
      "   database_table: 573\n",
      "   json_table: 6\n",
      "\n",
      "üéâ EMBEDDINGS GENERATED AND SAVED!\n",
      "You can now use these embeddings for semantic search, similarity, etc.\n"
     ]
    }
   ],
   "source": [
    "# üöÄ GENERATE EMBEDDINGS FOR ALL DATA\n",
    "# This cell will process all your data and generate embeddings\n",
    "\n",
    "# Set this to True to actually generate embeddings\n",
    "generate_embeddings = True\n",
    "\n",
    "if generate_embeddings:\n",
    "    print(\"=== DYNAMIC EMBEDDING GENERATION ===\\n\")\n",
    "\n",
    "    # Initialize the generator\n",
    "    embedding_generator = DynamicEmbeddingGenerator(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Process all data (automatically detects table structures)\n",
    "    embedding_generator.process_all_data(\"../data\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üìã PREVIEW OF GENERATED TEXTS:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Show examples of the generated texts\n",
    "    for i, (text, metadata) in enumerate(\n",
    "        zip(\n",
    "            embedding_generator.embeddings_data[\"texts\"][:5],\n",
    "            embedding_generator.embeddings_data[\"metadata\"][:5],\n",
    "        )\n",
    "    ):\n",
    "        print(f\"\\nüî∏ Example {i + 1} ({metadata['type']}):\")\n",
    "        print(\n",
    "            f\"   Source: {metadata.get('source_table', metadata.get('source_file', 'unknown'))}\"\n",
    "        )\n",
    "        print(f\"   Text: {text[:150]}...\")\n",
    "\n",
    "    # Generate the actual embeddings\n",
    "    embedding_generator.generate_embeddings()\n",
    "\n",
    "    # Save embeddings with Neo4j integration\n",
    "    try:\n",
    "        # Try to connect to Neo4j for integration\n",
    "        from notebooks.main import KnowledgeGraphBuilder, NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD\n",
    "        kg_builder = KnowledgeGraphBuilder(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "        \n",
    "        # Save with Neo4j integration\n",
    "        embedding_generator.save_embeddings_with_neo4j_integration(\n",
    "            \"../data/knowledge_graph_embeddings.pkl\", \n",
    "            kg_builder\n",
    "        )\n",
    "        kg_builder.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Neo4j integration failed: {e}\")\n",
    "        print(\"Saving embeddings without Neo4j integration...\")\n",
    "        # Fallback to regular save\n",
    "        embedding_generator.save_embeddings(\"../data/knowledge_graph_embeddings.pkl\")\n",
    "\n",
    "    print(\"\\nüéâ EMBEDDINGS GENERATED AND SAVED!\")\n",
    "    print(\"You can now use these embeddings for semantic search, similarity, etc.\")\n",
    "\n",
    "else:\n",
    "    print(\"‚è∏Ô∏è Embedding generation skipped (set generate_embeddings = True to run)\")\n",
    "    print(\"This process will:\")\n",
    "    print(\"1. üîç Automatically detect the structure of all your CSV tables\")\n",
    "    print(\"2. üìù Generate natural language descriptions for each row\")\n",
    "    print(\"3. üß† Create embeddings using SentenceTransformers\")\n",
    "    print(\"4. üíæ Save everything with rich metadata for retrieval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63ee23ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Semantic Search Engine ready!\n"
     ]
    }
   ],
   "source": [
    "# üîç SEMANTIC SEARCH AND SIMILARITY UTILITIES\n",
    "# Use the generated embeddings for intelligent queries\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EmbeddingSearchEngine:\n",
    "    def __init__(self, embeddings_path=\"knowledge_graph_embeddings.pkl\"):\n",
    "        \"\"\"Initialize search engine with pre-generated embeddings\"\"\"\n",
    "        self.generator = DynamicEmbeddingGenerator()\n",
    "        self.embeddings_data = self.generator.load_embeddings(embeddings_path)\n",
    "        self.embeddings = self.embeddings_data[\"embeddings\"]\n",
    "        self.metadata = self.embeddings_data[\"metadata\"]\n",
    "        self.texts = self.embeddings_data[\"texts\"]\n",
    "\n",
    "    def semantic_search(self, query, top_k=5, filter_type=None):\n",
    "        \"\"\"Search for similar content using semantic similarity\"\"\"\n",
    "        print(f\"üîç Searching for: '{query}'\")\n",
    "\n",
    "        # Generate embedding for query\n",
    "        query_embedding = self.generator.model.encode([query])\n",
    "\n",
    "        # Filter by type if specified\n",
    "        valid_indices = range(len(self.embeddings))\n",
    "        if filter_type:\n",
    "            valid_indices = [\n",
    "                i for i, meta in enumerate(self.metadata) if meta[\"type\"] == filter_type\n",
    "            ]\n",
    "            print(f\"   Filtering to {filter_type} only ({len(valid_indices)} items)\")\n",
    "\n",
    "        if not valid_indices:\n",
    "            print(\"No items match the filter criteria!\")\n",
    "            return []\n",
    "\n",
    "        # Calculate similarities\n",
    "        filtered_embeddings = self.embeddings[valid_indices]\n",
    "        similarities = cosine_similarity(query_embedding, filtered_embeddings)[0]\n",
    "\n",
    "        # Get top results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        for rank, idx in enumerate(top_indices):\n",
    "            original_idx = valid_indices[idx]\n",
    "            result = {\n",
    "                \"rank\": rank + 1,\n",
    "                \"similarity\": similarities[idx],\n",
    "                \"text\": self.texts[original_idx],\n",
    "                \"metadata\": self.metadata[original_idx],\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def find_similar_items(self, item_id, top_k=5):\n",
    "        \"\"\"Find items similar to a specific item in the database\"\"\"\n",
    "        # Find the item\n",
    "        item_idx = None\n",
    "        for i, meta in enumerate(self.metadata):\n",
    "            if meta[\"id\"] == item_id or str(meta.get(\"entity_id\", \"\")) == str(item_id):\n",
    "                item_idx = i\n",
    "                break\n",
    "\n",
    "        if item_idx is None:\n",
    "            print(f\"Item {item_id} not found!\")\n",
    "            return []\n",
    "\n",
    "        print(f\"üîç Finding items similar to: {self.texts[item_idx][:100]}...\")\n",
    "\n",
    "        # Calculate similarities with all other items\n",
    "        item_embedding = self.embeddings[item_idx : item_idx + 1]\n",
    "        similarities = cosine_similarity(item_embedding, self.embeddings)[0]\n",
    "\n",
    "        # Exclude the item itself\n",
    "        similarities[item_idx] = -1\n",
    "\n",
    "        # Get top results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        for rank, idx in enumerate(top_indices):\n",
    "            if similarities[idx] > 0:  # Only positive similarities\n",
    "                result = {\n",
    "                    \"rank\": rank + 1,\n",
    "                    \"similarity\": similarities[idx],\n",
    "                    \"text\": self.texts[idx],\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get statistics about the embedding database\"\"\"\n",
    "        stats = {\n",
    "            \"total_embeddings\": len(self.embeddings),\n",
    "            \"embedding_dimension\": self.embeddings.shape[1],\n",
    "            \"types\": {},\n",
    "        }\n",
    "\n",
    "        for meta in self.metadata:\n",
    "            type_key = meta[\"type\"]\n",
    "            stats[\"types\"][type_key] = stats[\"types\"].get(type_key, 0) + 1\n",
    "\n",
    "        return stats\n",
    "\n",
    "\n",
    "def demo_semantic_search():\n",
    "    \"\"\"Demonstrate semantic search capabilities\"\"\"\n",
    "    try:\n",
    "        # Initialize search engine\n",
    "        search_engine = EmbeddingSearchEngine(\"../data/knowledge_graph_embeddings.pkl\")\n",
    "\n",
    "        print(\"=== SEMANTIC SEARCH DEMO ===\\n\")\n",
    "\n",
    "        # Show statistics\n",
    "        stats = search_engine.get_statistics()\n",
    "        print(f\"üìä Database contains {stats['total_embeddings']} items:\")\n",
    "        for type_name, count in stats[\"types\"].items():\n",
    "            print(f\"   {type_name}: {count}\")\n",
    "\n",
    "        print(f\"\\nüß† Embedding dimension: {stats['embedding_dimension']}\")\n",
    "\n",
    "        # Example searches\n",
    "        example_queries = [\n",
    "            \"mountain bike frame\",\n",
    "            \"red helmet\",\n",
    "            \"road cycling equipment\",\n",
    "            \"vintage bicycle parts\",\n",
    "        ]\n",
    "\n",
    "        for query in example_queries:\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(f\"üîç QUERY: '{query}'\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "            results = search_engine.semantic_search(query, top_k=3)\n",
    "\n",
    "            for result in results:\n",
    "                print(\n",
    "                    f\"\\nüèÜ Rank {result['rank']} (similarity: {result['similarity']:.3f})\"\n",
    "                )\n",
    "                print(f\"   Type: {result['metadata']['type']}\")\n",
    "                print(f\"   Source: {result['metadata'].get('source_table', 'unknown')}\")\n",
    "                print(f\"   Text: {result['text'][:200]}...\")\n",
    "\n",
    "        # Example: Find similar items to a specific product\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(\"üîó FINDING SIMILAR ITEMS\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Try to find similar items to the first product\n",
    "        if search_engine.metadata:\n",
    "            first_item_id = search_engine.metadata[0][\"id\"]\n",
    "            similar_items = search_engine.find_similar_items(first_item_id, top_k=3)\n",
    "\n",
    "            for item in similar_items:\n",
    "                print(f\"\\nüîó Similarity: {item['similarity']:.3f}\")\n",
    "                print(f\"   Type: {item['metadata']['type']}\")\n",
    "                print(f\"   Text: {item['text'][:200]}...\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Embeddings file not found!\")\n",
    "        print(\"Run the embedding generation cell first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "\n",
    "print(\"üîç Semantic Search Engine ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5522298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "üìÇ Loading embeddings from ../data/knowledge_graph_embeddings.pkl\n",
      "‚úÖ Loaded 579 embeddings\n",
      "=== HYBRID SEARCH DEMO ===\n",
      "\n",
      "üîç Example 1: Semantic search with price filter\n",
      "Query: 'mountain bike' with price range $100-$500\n",
      "üîç Hybrid search for: 'mountain bike'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0692c00b8c6e48c4bbcb6fa51024993e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Similarity: 0.652\n",
      "   Text: Table: ProductDescription. Description: Top-of-the-line competition mountain bike. Performance-enhan...\n",
      "\n",
      "2. Similarity: 0.569\n",
      "   Text: Table: ProductDescription. Description: Replacement mountain wheel for the casual to serious rider.....\n",
      "\n",
      "3. Similarity: 0.566\n",
      "   Text: Table: ProductDescription. Description: Serious back-country riding. Perfect for all levels of compe...\n",
      "\n",
      "============================================================\n",
      "üîó Example 2: Find similar products in same category\n",
      "Finding products similar to Product ID: 680\n",
      "üîó Finding similar products to 680\n",
      "\n",
      "============================================================\n",
      "üåê Example 3: Cross-modal semantic search\n",
      "Finding 'technical specifications' across products AND documents\n",
      "üîç Hybrid search for: 'technical specifications materials'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a7b870442c4e02a99939e5d520c638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Similarity: 0.483\n",
      "   Type: database_table\n",
      "   Source: ../data/ProductDescription.csv\n",
      "   Text: Table: ProductDescription. Description: Lightweight aluminum alloy construction.. ModifiedDate: 2007-06-01 00:00:00.000. ProductDescriptionID: 851...\n",
      "\n",
      "2. Similarity: 0.474\n",
      "   Type: database_table\n",
      "   Source: ../data/ProductDescription.csv\n",
      "   Text: Table: ProductDescription. Description: Sturdy alloy features a quick-release hub.. ModifiedDate: 2007-06-01 00:00:00.000. ProductDescriptionID: 690...\n",
      "\n",
      "3. Similarity: 0.470\n",
      "   Type: database_table\n",
      "   Source: ../data/ProductDescription.csv\n",
      "   Text: Table: ProductDescription. Description: Travel in style and comfort. Designed for maximum comfort and safety. Wide gear range takes on all hills. High...\n",
      "\n",
      "4. Similarity: 0.452\n",
      "   Type: database_table\n",
      "   Source: ../data/ProductDescription.csv\n",
      "   Text: Table: ProductDescription. Description: Our lightest and best quality aluminum frame made from the newest alloy; it is welded and heat-treated for str...\n",
      "\n",
      "5. Similarity: 0.444\n",
      "   Type: database_table\n",
      "   Source: ../data/ProductDescription.csv\n",
      "   Text: Table: ProductDescription. Description: Our best value utilizing the same, ground-breaking frame technology as the ML aluminum frame.. ModifiedDate: 2...\n",
      "\n",
      "============================================================\n",
      "üéØ HYBRID SEARCH CAPABILITIES:\n",
      "‚úÖ Semantic similarity search across all data types\n",
      "‚úÖ Graph-based filtering (category, price, relationships)\n",
      "‚úÖ Cross-modal search (products + documents + JSON)\n",
      "‚úÖ Relationship-aware results\n",
      "‚úÖ Fast embedding lookup with graph context\n"
     ]
    }
   ],
   "source": [
    "# üîó HYBRID NEO4J + EMBEDDINGS SEARCH DEMO\n",
    "# This demonstrates the power of combining graph queries with semantic search\n",
    "\n",
    "def demo_hybrid_search():\n",
    "    \"\"\"Demonstrate hybrid search capabilities\"\"\"\n",
    "    try:\n",
    "        # Initialize components\n",
    "        kg_builder = KnowledgeGraphBuilder(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "        embedding_generator = DynamicEmbeddingGenerator()\n",
    "        embedding_generator.load_embeddings(\"../data/knowledge_graph_embeddings.pkl\")\n",
    "        \n",
    "        # Create hybrid search engine\n",
    "        hybrid_search = HybridSearchEngine(kg_builder, embedding_generator)\n",
    "        \n",
    "        print(\"=== HYBRID SEARCH DEMO ===\\n\")\n",
    "        \n",
    "        # Example 1: Semantic search with graph filtering\n",
    "        print(\"üîç Example 1: Semantic search with price filter\")\n",
    "        print(\"Query: 'mountain bike' with price range $100-$500\")\n",
    "        \n",
    "        results = hybrid_search.semantic_graph_search(\n",
    "            \"mountain bike\", \n",
    "            top_k=3,\n",
    "            graph_filter={\"price_range\": [100, 500]}\n",
    "        )\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. Similarity: {result['similarity']:.3f}\")\n",
    "            if result['graph_data']:\n",
    "                print(f\"   Product: {result['graph_data']['name']}\")\n",
    "                print(f\"   Category: {result['graph_data'].get('category_name', 'Unknown')}\")\n",
    "                print(f\"   Price: ${result['graph_data'].get('list_price', 0)}\")\n",
    "                print(f\"   Relationships: {result['relationships']}\")\n",
    "            else:\n",
    "                print(f\"   Text: {result['text'][:100]}...\")\n",
    "        \n",
    "        # Example 2: Find similar products in same category\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üîó Example 2: Find similar products in same category\")\n",
    "        \n",
    "        # Get first product ID from embeddings\n",
    "        first_product_metadata = next(\n",
    "            (m for m in embedding_generator.embeddings_data[\"metadata\"] \n",
    "             if m[\"type\"] == \"database_table\" and m[\"source_table\"] == \"Product\" and \"entity_id\" in m),\n",
    "            None\n",
    "        )\n",
    "        \n",
    "        if first_product_metadata:\n",
    "            product_id = first_product_metadata[\"entity_id\"]\n",
    "            print(f\"Finding products similar to Product ID: {product_id}\")\n",
    "            \n",
    "            similar_products = hybrid_search.find_similar_products_in_category(\n",
    "                product_id, \n",
    "                category=\"Road Bikes\",  # Filter to specific category\n",
    "                top_k=3\n",
    "            )\n",
    "            \n",
    "            for i, product in enumerate(similar_products, 1):\n",
    "                print(f\"\\n{i}. Similarity: {product['similarity']:.3f}\")\n",
    "                print(f\"   Product: {product['name']} (ID: {product['product_id']})\")\n",
    "                print(f\"   Category: {product['category']}\")\n",
    "                print(f\"   Price: ${product['price']}\")\n",
    "        \n",
    "        # Example 3: Pure semantic search across all data types\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üåê Example 3: Cross-modal semantic search\")\n",
    "        print(\"Finding 'technical specifications' across products AND documents\")\n",
    "        \n",
    "        results = hybrid_search.semantic_graph_search(\n",
    "            \"technical specifications materials\", \n",
    "            top_k=5\n",
    "        )\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. Similarity: {result['similarity']:.3f}\")\n",
    "            print(f\"   Type: {result['metadata']['type']}\")\n",
    "            if result['graph_data']:\n",
    "                print(f\"   Product: {result['graph_data']['name']}\")\n",
    "            else:\n",
    "                print(f\"   Source: {result['metadata'].get('source_file', 'Unknown')}\")\n",
    "            print(f\"   Text: {result['text'][:150]}...\")\n",
    "        \n",
    "        kg_builder.close()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üéØ HYBRID SEARCH CAPABILITIES:\")\n",
    "        print(\"‚úÖ Semantic similarity search across all data types\")\n",
    "        print(\"‚úÖ Graph-based filtering (category, price, relationships)\")\n",
    "        print(\"‚úÖ Cross-modal search (products + documents + JSON)\")\n",
    "        print(\"‚úÖ Relationship-aware results\")\n",
    "        print(\"‚úÖ Fast embedding lookup with graph context\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Hybrid search demo failed: {e}\")\n",
    "        print(\"Make sure Neo4j is running and embeddings are generated!\")\n",
    "\n",
    "# Set to True to run the hybrid search demo\n",
    "run_hybrid_demo = True\n",
    "\n",
    "if run_hybrid_demo:\n",
    "    demo_hybrid_search()\n",
    "else:\n",
    "    print(\"üîó HYBRID SEARCH DEMO READY\")\n",
    "    print(\"Set run_hybrid_demo = True to see the hybrid Neo4j + embeddings search!\")\n",
    "    print()\n",
    "    print(\"This demo will show:\")\n",
    "    print(\"1. üîç Semantic search with graph filters (e.g., price range, category)\")\n",
    "    print(\"2. üîó Find similar products within specific categories\")\n",
    "    print(\"3. üåê Cross-modal search across products, documents, and JSON data\")\n",
    "    print(\"4. üéØ Relationship-aware results combining graph and vector data\")\n",
    "    print()\n",
    "    print(\"Benefits of this hybrid approach:\")\n",
    "    print(\"‚Ä¢ üöÄ Fast semantic search using embeddings\")\n",
    "    print(\"‚Ä¢ üéØ Precise filtering using graph relationships\")\n",
    "    print(\"‚Ä¢ üîÑ Best of both worlds: similarity + structure\")\n",
    "    print(\"‚Ä¢ üìä Rich context from both vector and graph data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf70b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Intelligent Query Parser created\n"
     ]
    }
   ],
   "source": [
    "# üß† INTELLIGENT QUERY PARSING SYSTEM\n",
    "# This system parses user queries to extract relevant entities using LLM\n",
    "\n",
    "import json\n",
    "from typing import Dict, Any, List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Define structured output format for query parsing\n",
    "class ProductEntity(BaseModel):\n",
    "    name: Optional[str] = None\n",
    "    features: List[str] = Field(default_factory=list)\n",
    "    category: Optional[str] = None\n",
    "\n",
    "class DocumentEntity(BaseModel):\n",
    "    type: Optional[str] = None\n",
    "    name: Optional[str] = None\n",
    "\n",
    "class RelationshipEntity(BaseModel):\n",
    "    type: Optional[str] = None\n",
    "    direction: Optional[str] = None\n",
    "\n",
    "class QueryEntities(BaseModel):\n",
    "    product: ProductEntity = Field(default_factory=ProductEntity)\n",
    "    document: DocumentEntity = Field(default_factory=DocumentEntity)\n",
    "    relationship: RelationshipEntity = Field(default_factory=RelationshipEntity)\n",
    "\n",
    "class IntelligentQueryParser:\n",
    "    def __init__(self, base_url=\"http://localhost:11434/v1\", model_name=\"gemma3:1b\"):\n",
    "        \"\"\"Initialize the query parser with local LLM\"\"\"\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=\"local-llm\",  # dummy key for local LLM\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def parse_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Parse user query to extract relevant entities\n",
    "        \n",
    "        Args:\n",
    "            query: User's natural language query\n",
    "            \n",
    "        Returns:\n",
    "            Dict with extracted entities: product, document, relationship\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use structured output parsing\n",
    "            completion = self.client.beta.chat.completions.parse(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"\n",
    "                        You are a query parser for a product knowledge graph system.\n",
    "                        Extract relevant entities from user queries about products, documents, and relationships.\n",
    "                        \n",
    "                        Extract these entities:\n",
    "                        1. Product:\n",
    "                           - name: main product name or identifier\n",
    "                           - features: list of specific features (color, size, material, etc.)\n",
    "                           - category: product category (bikes, components, clothing, accessories)\n",
    "                        \n",
    "                        2. Document:\n",
    "                           - type: document type (manual, specification, guide, etc.)\n",
    "                           - name: specific document name\n",
    "                        \n",
    "                        3. Relationship:\n",
    "                           - type: relationship type (compatible_with, similar_to, part_of, etc.)\n",
    "                           - direction: relationship direction (incoming, outgoing, bidirectional)\n",
    "                        \n",
    "                        If an entity is not mentioned in the query, set its values to null.\n",
    "                        Extract information in English regardless of input language.\n",
    "                        \"\"\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": query},\n",
    "                ],\n",
    "                response_format=QueryEntities,\n",
    "            )\n",
    "            \n",
    "            # Parse the response\n",
    "            result_str = completion.choices[0].message.content\n",
    "            result_dict = json.loads(result_str)\n",
    "            return QueryEntities(**result_dict).model_dump()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error parsing query: {e}\")\n",
    "            # Return empty structure on error\n",
    "            return {\n",
    "                \"product\": {\"name\": None, \"features\": [], \"category\": None},\n",
    "                \"document\": {\"type\": None, \"name\": None},\n",
    "                \"relationship\": {\"type\": None, \"direction\": None},\n",
    "            }\n",
    "    \n",
    "    def create_search_text(self, parsed_query: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Convert parsed query entities back to searchable text\n",
    "        \n",
    "        Args:\n",
    "            parsed_query: Output from parse_query()\n",
    "            \n",
    "        Returns:\n",
    "            Optimized text for embedding search\n",
    "        \"\"\"\n",
    "        search_parts = []\n",
    "        \n",
    "        # Add product information\n",
    "        product = parsed_query.get(\"product\", {})\n",
    "        if product.get(\"name\"):\n",
    "            search_parts.append(f\"Product: {product['name']}\")\n",
    "        if product.get(\"category\"):\n",
    "            search_parts.append(f\"Category: {product['category']}\")\n",
    "        if product.get(\"features\"):\n",
    "            features_text = \", \".join(product[\"features\"])\n",
    "            search_parts.append(f\"Features: {features_text}\")\n",
    "        \n",
    "        # Add document information\n",
    "        document = parsed_query.get(\"document\", {})\n",
    "        if document.get(\"type\"):\n",
    "            search_parts.append(f\"Document type: {document['type']}\")\n",
    "        if document.get(\"name\"):\n",
    "            search_parts.append(f\"Document: {document['name']}\")\n",
    "        \n",
    "        # Add relationship information\n",
    "        relationship = parsed_query.get(\"relationship\", {})\n",
    "        if relationship.get(\"type\"):\n",
    "            search_parts.append(f\"Relationship: {relationship['type']}\")\n",
    "        \n",
    "        return \". \".join(search_parts) if search_parts else \"\"\n",
    "\n",
    "print(\"‚úÖ Intelligent Query Parser created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11d4e7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding RAG System created\n"
     ]
    }
   ],
   "source": [
    "# üîç COMPLETE EMBEDDING-BASED RAG SEARCH SYSTEM\n",
    "# This system processes user queries and finds similar content using embeddings\n",
    "\n",
    "class EmbeddingRAGSystem:\n",
    "    def __init__(self, embeddings_path=\"knowledge_graph_embeddings.pkl\", \n",
    "                 embedding_model_name='all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with embeddings and query parsing\n",
    "        \n",
    "        Args:\n",
    "            embeddings_path: Path to saved embeddings file\n",
    "            embedding_model_name: Name of the sentence transformer model\n",
    "        \"\"\"\n",
    "        print(\"üöÄ Initializing Embedding RAG System...\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.query_parser = IntelligentQueryParser()\n",
    "        self.embedding_generator = DynamicEmbeddingGenerator(embedding_model_name)\n",
    "        \n",
    "        # Load pre-generated embeddings\n",
    "        self.load_embeddings(embeddings_path)\n",
    "        \n",
    "        print(\"‚úÖ RAG System ready!\")\n",
    "    \n",
    "    def load_embeddings(self, embeddings_path):\n",
    "        \"\"\"Load embeddings from file\"\"\"\n",
    "        try:\n",
    "            self.embeddings_data = self.embedding_generator.load_embeddings(embeddings_path)\n",
    "            self.embeddings_matrix = np.array(self.embeddings_data['embeddings'])\n",
    "            self.texts = self.embeddings_data['texts']\n",
    "            self.metadata = self.embeddings_data['metadata']\n",
    "            \n",
    "            print(f\"üìä Loaded {len(self.texts)} embeddings\")\n",
    "            print(f\"üî¢ Embedding dimensions: {self.embeddings_matrix.shape[1]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading embeddings: {e}\")\n",
    "            self.embeddings_matrix = np.array([])\n",
    "            self.texts = []\n",
    "            self.metadata = []\n",
    "    \n",
    "    def process_query(self, user_query: str, top_k: int = 5, \n",
    "                     similarity_threshold: float = 0.3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete RAG pipeline: parse query ‚Üí embed ‚Üí search ‚Üí return results\n",
    "        \n",
    "        Args:\n",
    "            user_query: Natural language query from user\n",
    "            top_k: Number of top results to return\n",
    "            similarity_threshold: Minimum similarity score to include\n",
    "            \n",
    "        Returns:\n",
    "            Dict with parsed query, search results, and metadata\n",
    "        \"\"\"\n",
    "        print(f\"üîç Processing query: '{user_query}'\")\n",
    "        \n",
    "        # Step 1: Parse the query using LLM\n",
    "        print(\"üìù Step 1: Parsing query...\")\n",
    "        parsed_query = self.query_parser.parse_query(user_query)\n",
    "        \n",
    "        # Step 2: Create optimized search text\n",
    "        search_text = self.query_parser.create_search_text(parsed_query)\n",
    "        if not search_text:\n",
    "            search_text = user_query  # Fallback to original query\n",
    "        \n",
    "        print(f\"üéØ Search text: '{search_text}'\")\n",
    "        \n",
    "        # Step 3: Generate embedding for search text\n",
    "        print(\"üß† Step 2: Generating query embedding...\")\n",
    "        query_embedding = self.embedding_generator.model.encode([search_text])\n",
    "        \n",
    "        # Step 4: Find similar embeddings\n",
    "        print(\"üîç Step 3: Searching for similar content...\")\n",
    "        search_results = self.find_similar_content(\n",
    "            query_embedding[0], \n",
    "            top_k=top_k, \n",
    "            similarity_threshold=similarity_threshold\n",
    "        )\n",
    "        \n",
    "        # Step 5: Compile comprehensive results\n",
    "        results = {\n",
    "            \"original_query\": user_query,\n",
    "            \"parsed_query\": parsed_query,\n",
    "            \"search_text\": search_text,\n",
    "            \"results\": search_results,\n",
    "            \"summary\": self.create_results_summary(search_results)\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(search_results)} relevant results\")\n",
    "        return results\n",
    "    \n",
    "    def find_similar_content(self, query_embedding, top_k=5, similarity_threshold=0.3):\n",
    "        \"\"\"\n",
    "        Find content similar to query embedding\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: Numpy array of query embedding\n",
    "            top_k: Number of top results\n",
    "            similarity_threshold: Minimum similarity score\n",
    "            \n",
    "        Returns:\n",
    "            List of similar content with metadata\n",
    "        \"\"\"\n",
    "        if len(self.embeddings_matrix) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = cosine_similarity([query_embedding], self.embeddings_matrix)[0]\n",
    "        \n",
    "        # Get top-k most similar indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        # Filter by threshold and compile results\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            similarity_score = similarities[idx]\n",
    "            \n",
    "            if similarity_score >= similarity_threshold:\n",
    "                result = {\n",
    "                    \"similarity_score\": float(similarity_score),\n",
    "                    \"content\": self.texts[idx],\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"index\": int(idx)\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_results_summary(self, search_results):\n",
    "        \"\"\"Create a summary of search results\"\"\"\n",
    "        if not search_results:\n",
    "            return \"No relevant results found.\"\n",
    "        \n",
    "        # Count results by type\n",
    "        type_counts = {}\n",
    "        categories = set()\n",
    "        \n",
    "        for result in search_results:\n",
    "            metadata = result[\"metadata\"]\n",
    "            result_type = metadata.get(\"type\", \"unknown\")\n",
    "            type_counts[result_type] = type_counts.get(result_type, 0) + 1\n",
    "            \n",
    "            if result_type == \"database_table\":\n",
    "                categories.add(metadata.get(\"table_name\", \"unknown\"))\n",
    "        \n",
    "        # Create summary\n",
    "        summary_parts = [\n",
    "            f\"Found {len(search_results)} relevant results\",\n",
    "            f\"Average similarity: {np.mean([r['similarity_score'] for r in search_results]):.3f}\"\n",
    "        ]\n",
    "        \n",
    "        if type_counts:\n",
    "            type_summary = \", \".join([f\"{count} {type_}\" for type_, count in type_counts.items()])\n",
    "            summary_parts.append(f\"Types: {type_summary}\")\n",
    "        \n",
    "        if categories:\n",
    "            summary_parts.append(f\"Categories: {', '.join(sorted(categories))}\")\n",
    "        \n",
    "        return \". \".join(summary_parts)\n",
    "    \n",
    "    def search_by_category(self, user_query: str, category_filter: str, top_k: int = 5):\n",
    "        \"\"\"\n",
    "        Search within a specific category only\n",
    "        \n",
    "        Args:\n",
    "            user_query: User's query\n",
    "            category_filter: Category to filter by (e.g., \"database_table\", \"json_table\")\n",
    "            top_k: Number of results\n",
    "            \n",
    "        Returns:\n",
    "            Filtered search results\n",
    "        \"\"\"\n",
    "        # Get all results first\n",
    "        all_results = self.process_query(user_query, top_k=50)  # Get more to filter\n",
    "        \n",
    "        # Filter by category\n",
    "        filtered_results = []\n",
    "        for result in all_results[\"results\"]:\n",
    "            if result[\"metadata\"].get(\"type\") == category_filter:\n",
    "                filtered_results.append(result)\n",
    "                if len(filtered_results) >= top_k:\n",
    "                    break\n",
    "        \n",
    "        # Update results\n",
    "        all_results[\"results\"] = filtered_results\n",
    "        all_results[\"summary\"] = f\"Category-filtered search: {self.create_results_summary(filtered_results)}\"\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def get_content_statistics(self):\n",
    "        \"\"\"Get statistics about the loaded content\"\"\"\n",
    "        if not self.metadata:\n",
    "            return \"No content loaded.\"\n",
    "        \n",
    "        # Count by type\n",
    "        type_counts = {}\n",
    "        table_counts = {}\n",
    "        \n",
    "        for meta in self.metadata:\n",
    "            content_type = meta.get(\"type\", \"unknown\")\n",
    "            type_counts[content_type] = type_counts.get(content_type, 0) + 1\n",
    "            \n",
    "            if content_type == \"database_table\":\n",
    "                table_name = meta.get(\"table_name\", \"unknown\")\n",
    "                table_counts[table_name] = table_counts.get(table_name, 0) + 1\n",
    "        \n",
    "        stats = {\n",
    "            \"total_entries\": len(self.metadata),\n",
    "            \"content_types\": type_counts,\n",
    "            \"database_tables\": table_counts,\n",
    "            \"embedding_dimensions\": self.embeddings_matrix.shape[1] if len(self.embeddings_matrix) > 0 else 0\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "print(\"‚úÖ Embedding RAG System created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaa703fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ COMPLETE RAG SYSTEM DEMO\n",
      "============================================================\n",
      "üöÄ Initializing Embedding RAG System...\n",
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "üìÇ Loading embeddings from ../data/knowledge_graph_embeddings.pkl\n",
      "‚úÖ Loaded 579 embeddings\n",
      "üìä Loaded 579 embeddings\n",
      "üî¢ Embedding dimensions: 384\n",
      "‚úÖ RAG System ready!\n",
      "\n",
      "üìä CONTENT STATISTICS:\n",
      "  total_entries: 579\n",
      "  content_types:\n",
      "    - database_table: 573\n",
      "    - json_table: 6\n",
      "  database_tables:\n",
      "    - unknown: 573\n",
      "  embedding_dimensions: 384\n",
      "\n",
      "============================================================\n",
      "üîç TESTING DIFFERENT QUERIES\n",
      "============================================================\n",
      "\n",
      "üéØ TEST 1: 'Find information about mountain bikes'\n",
      "--------------------------------------------------\n",
      "üîç Processing query: 'Find information about mountain bikes'\n",
      "üìù Step 1: Parsing query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error parsing query: Error code: 404 - {'error': {'message': 'model \"gemma2:2b\" not found, try pulling it first', 'type': 'api_error', 'param': None, 'code': None}}\n",
      "üéØ Search text: 'Find information about mountain bikes'\n",
      "üß† Step 2: Generating query embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2bcab4a860f440ea116884676911017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Step 3: Searching for similar content...\n",
      "‚úÖ Found 3 relevant results\n",
      "üìù PARSED QUERY:\n",
      "\n",
      "üîç SEARCH RESULTS (3 found):\n",
      "  1. Score: 0.567\n",
      "     Type: json_table\n",
      "     Content: Document: Mountain Bike Manual. Contains hierarchical information. Gear Usage: Low Gear (1-3): Steep climbs. Gear Usage: Mid Gear (4-6): Flat trails. ...\n",
      "\n",
      "  2. Score: 0.565\n",
      "     Type: database_table\n",
      "     Content: Table: ProductDescription. Description: Top-of-the-line competition mountain bike. Performance-enhancing options include the innovative HL Frame, supe...\n",
      "\n",
      "  3. Score: 0.547\n",
      "     Type: database_table\n",
      "     Content: Table: ProductCategory. Name: Mountain Bikes. ModifiedDate: 2002-06-01 00:00:00.000. ProductCategoryID: 5. ParentProductCategoryID: 1.0\n",
      "\n",
      "üìã SUMMARY: Found 3 relevant results. Average similarity: 0.560. Types: 1 json_table, 2 database_table. Categories: unknown\n",
      "\n",
      "==============================\n",
      "\n",
      "\n",
      "üéØ TEST 2: 'Show me black bicycle components'\n",
      "--------------------------------------------------\n",
      "üîç Processing query: 'Show me black bicycle components'\n",
      "üìù Step 1: Parsing query...\n",
      "‚ö†Ô∏è Error parsing query: Error code: 404 - {'error': {'message': 'model \"gemma2:2b\" not found, try pulling it first', 'type': 'api_error', 'param': None, 'code': None}}\n",
      "üéØ Search text: 'Show me black bicycle components'\n",
      "üß† Step 2: Generating query embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb17f5cacde4657a3d081c15a678af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Step 3: Searching for similar content...\n",
      "‚úÖ Found 3 relevant results\n",
      "üìù PARSED QUERY:\n",
      "\n",
      "üîç SEARCH RESULTS (3 found):\n",
      "  1. Score: 0.524\n",
      "     Type: database_table\n",
      "     Content: Table: ProductDescription. Description: Clipless pedals - aluminum.. ModifiedDate: 2007-06-01 00:00:00.000. ProductDescriptionID: 850\n",
      "\n",
      "  2. Score: 0.497\n",
      "     Type: database_table\n",
      "     Content: Table: ProductDescription. Description: Value-priced bike with many features of our top-of-the-line models. Has the same light, stiff frame, and the q...\n",
      "\n",
      "  3. Score: 0.496\n",
      "     Type: database_table\n",
      "     Content: Table: ProductDescription. Description: Same technology as all of our Road series bikes.  Perfect all-around bike for road or racing.. ModifiedDate: 2...\n",
      "\n",
      "üìã SUMMARY: Found 3 relevant results. Average similarity: 0.505. Types: 3 database_table. Categories: unknown\n",
      "\n",
      "==============================\n",
      "\n",
      "\n",
      "üéØ TEST 3: 'What are the specifications for bike frames?'\n",
      "--------------------------------------------------\n",
      "üîç Processing query: 'What are the specifications for bike frames?'\n",
      "üìù Step 1: Parsing query...\n",
      "‚ö†Ô∏è Error parsing query: Error code: 404 - {'error': {'message': 'model \"gemma2:2b\" not found, try pulling it first', 'type': 'api_error', 'param': None, 'code': None}}\n",
      "üéØ Search text: 'What are the specifications for bike frames?'\n",
      "üß† Step 2: Generating query embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c4a171fc6f4b79809e17b069264274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Step 3: Searching for similar content...\n",
      "‚úÖ Found 3 relevant results\n",
      "üìù PARSED QUERY:\n",
      "\n",
      "üîç SEARCH RESULTS (3 found):\n",
      "  1. Score: 0.614\n",
      "     Type: database_table\n",
      "     Content: Table: ProductDescription. Description: Same technology as all of our Road series bikes, but the frame is sized for a woman.  Perfect all-around bike ...\n",
      "\n",
      "  2. Score: 0.561\n",
      "     Type: database_table\n",
      "     Content: Table: ProductDescription. Description: Lightweight butted aluminum frame provides a more upright riding position for a trip around town.  Our ground-...\n",
      "\n",
      "  3. Score: 0.533\n",
      "     Type: database_table\n",
      "     Content: Table: ProductDescription. Description: Value-priced bike with many features of our top-of-the-line models. Has the same light, stiff frame, and the q...\n",
      "\n",
      "üìã SUMMARY: Found 3 relevant results. Average similarity: 0.569. Types: 3 database_table. Categories: unknown\n",
      "\n",
      "==============================\n",
      "\n",
      "\n",
      "üéØ TEST 4: 'Find products with size 48 or similar'\n",
      "--------------------------------------------------\n",
      "üîç Processing query: 'Find products with size 48 or similar'\n",
      "üìù Step 1: Parsing query...\n",
      "‚ö†Ô∏è Error parsing query: Error code: 404 - {'error': {'message': 'model \"gemma2:2b\" not found, try pulling it first', 'type': 'api_error', 'param': None, 'code': None}}\n",
      "üéØ Search text: 'Find products with size 48 or similar'\n",
      "üß† Step 2: Generating query embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8564ae792336473ab0540b3d7fe8ceb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Step 3: Searching for similar content...\n",
      "‚úÖ Found 3 relevant results\n",
      "üìù PARSED QUERY:\n",
      "\n",
      "üîç SEARCH RESULTS (3 found):\n",
      "  1. Score: 0.440\n",
      "     Type: database_table\n",
      "     Content: Table: Product. Name: HL Mountain Frame - Silver, 48. ProductNumber: FR-M94S-52. Color: Silver. Size: 48. SellStartDate: 2005-07-01 00:00:00.000. Sell...\n",
      "\n",
      "  2. Score: 0.440\n",
      "     Type: database_table\n",
      "     Content: Table: Product. Name: LL Road Frame - Red, 48. ProductNumber: FR-R38R-48. Color: Red. Size: 48. SellStartDate: 2005-07-01 00:00:00.000. SellEndDate: 2...\n",
      "\n",
      "  3. Score: 0.438\n",
      "     Type: database_table\n",
      "     Content: Table: Product. Name: LL Road Frame - Red, 52. ProductNumber: FR-R38R-52. Color: Red. Size: 52. SellStartDate: 2005-07-01 00:00:00.000. SellEndDate: 2...\n",
      "\n",
      "üìã SUMMARY: Found 3 relevant results. Average similarity: 0.439. Types: 3 database_table. Categories: unknown\n",
      "\n",
      "==============================\n",
      "\n",
      "\n",
      "üéØ TEST 5: 'Show me technical manuals for bikes'\n",
      "--------------------------------------------------\n",
      "üîç Processing query: 'Show me technical manuals for bikes'\n",
      "üìù Step 1: Parsing query...\n",
      "‚ö†Ô∏è Error parsing query: Error code: 404 - {'error': {'message': 'model \"gemma2:2b\" not found, try pulling it first', 'type': 'api_error', 'param': None, 'code': None}}\n",
      "üéØ Search text: 'Show me technical manuals for bikes'\n",
      "üß† Step 2: Generating query embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a23d6f580e4e8fb0241321551e64ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Step 3: Searching for similar content...\n",
      "‚úÖ Found 3 relevant results\n",
      "üìù PARSED QUERY:\n",
      "\n",
      "üîç SEARCH RESULTS (3 found):\n",
      "  1. Score: 0.550\n",
      "     Type: database_table\n",
      "     Content: Table: ProductDescription. Description: Same technology as all of our Road series bikes.  Perfect all-around bike for road or racing.. ModifiedDate: 2...\n",
      "\n",
      "  2. Score: 0.488\n",
      "     Type: json_table\n",
      "     Content: Document: Mountain Bike Manual. Contains hierarchical information. Gear Usage: Low Gear (1-3): Steep climbs. Gear Usage: Mid Gear (4-6): Flat trails. ...\n",
      "\n",
      "  3. Score: 0.486\n",
      "     Type: database_table\n",
      "     Content: Table: ProductDescription. Description: This bike is ridden by race winners. Developed with the Adventure Works Cycles professional race team, it has ...\n",
      "\n",
      "üìã SUMMARY: Found 3 relevant results. Average similarity: 0.508. Types: 2 database_table, 1 json_table. Categories: unknown\n",
      "\n",
      "==============================\n",
      "\n",
      "üéØ CATEGORY-FILTERED SEARCH DEMO:\n",
      "----------------------------------------\n",
      "üîç Processing query: 'bike components'\n",
      "üìù Step 1: Parsing query...\n",
      "‚ö†Ô∏è Error parsing query: Error code: 404 - {'error': {'message': 'model \"gemma2:2b\" not found, try pulling it first', 'type': 'api_error', 'param': None, 'code': None}}\n",
      "üéØ Search text: 'bike components'\n",
      "üß† Step 2: Generating query embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2f01d76cb042208388643988c95c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Step 3: Searching for similar content...\n",
      "‚úÖ Found 50 relevant results\n",
      "Results for 'bike components' in database tables:\n",
      "  - unknown: 0.526\n",
      "  - unknown: 0.525\n",
      "  - unknown: 0.503\n",
      "\n",
      "‚úÖ RAG SYSTEM DEMO COMPLETED SUCCESSFULLY!\n"
     ]
    }
   ],
   "source": [
    "# üéØ COMPLETE RAG SYSTEM DEMO\n",
    "# This demonstrates the full pipeline: query parsing ‚Üí embedding ‚Üí search ‚Üí results\n",
    "\n",
    "def demo_rag_system():\n",
    "    \"\"\"Demonstrate the complete RAG system with various queries\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üöÄ COMPLETE RAG SYSTEM DEMO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize the RAG system\n",
    "    try:\n",
    "        rag_system = EmbeddingRAGSystem(\n",
    "            embeddings_path=\"../data/knowledge_graph_embeddings.pkl\",\n",
    "            embedding_model_name='all-MiniLM-L6-v2'\n",
    "        )\n",
    "        \n",
    "        # Show content statistics\n",
    "        print(\"\\nüìä CONTENT STATISTICS:\")\n",
    "        stats = rag_system.get_content_statistics()\n",
    "        for key, value in stats.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f\"  {key}:\")\n",
    "                for sub_key, sub_value in value.items():\n",
    "                    print(f\"    - {sub_key}: {sub_value}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Test queries\n",
    "        test_queries = [\n",
    "            \"Find information about mountain bikes\",\n",
    "            \"Show me black bicycle components\", \n",
    "            \"What are the specifications for bike frames?\",\n",
    "            \"Find products with size 48 or similar\",\n",
    "            \"Show me technical manuals for bikes\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üîç TESTING DIFFERENT QUERIES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, query in enumerate(test_queries, 1):\n",
    "            print(f\"\\nüéØ TEST {i}: '{query}'\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Process the query\n",
    "            results = rag_system.process_query(query, top_k=3, similarity_threshold=0.2)\n",
    "            \n",
    "            # Show parsed query\n",
    "            print(\"üìù PARSED QUERY:\")\n",
    "            parsed = results[\"parsed_query\"]\n",
    "            for entity_type, entity_data in parsed.items():\n",
    "                if any(entity_data.values() if isinstance(entity_data, dict) else [entity_data]):\n",
    "                    print(f\"  {entity_type}: {entity_data}\")\n",
    "            \n",
    "            # Show search results\n",
    "            print(f\"\\nüîç SEARCH RESULTS ({len(results['results'])} found):\")\n",
    "            for j, result in enumerate(results[\"results\"], 1):\n",
    "                score = result[\"similarity_score\"]\n",
    "                content = result[\"content\"][:150] + \"...\" if len(result[\"content\"]) > 150 else result[\"content\"]\n",
    "                metadata = result[\"metadata\"]\n",
    "                \n",
    "                print(f\"  {j}. Score: {score:.3f}\")\n",
    "                print(f\"     Type: {metadata.get('type', 'unknown')}\")\n",
    "                if metadata.get('table_name'):\n",
    "                    print(f\"     Table: {metadata['table_name']}\")\n",
    "                print(f\"     Content: {content}\")\n",
    "                print()\n",
    "            \n",
    "            print(f\"üìã SUMMARY: {results['summary']}\")\n",
    "            print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "        \n",
    "        # Demonstrate category filtering\n",
    "        print(\"üéØ CATEGORY-FILTERED SEARCH DEMO:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        category_results = rag_system.search_by_category(\n",
    "            \"bike components\", \n",
    "            category_filter=\"database_table\", \n",
    "            top_k=3\n",
    "        )\n",
    "        \n",
    "        print(f\"Results for 'bike components' in database tables:\")\n",
    "        for result in category_results[\"results\"]:\n",
    "            print(f\"  - {result['metadata'].get('table_name', 'unknown')}: {result['similarity_score']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ RAG SYSTEM DEMO COMPLETED SUCCESSFULLY!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in RAG demo: {e}\")\n",
    "        print(\"Make sure you have:\")\n",
    "        print(\"1. Generated embeddings (run the embedding generation cell)\")\n",
    "        print(\"2. Local LLM running on localhost:11434\")\n",
    "        print(\"3. All required libraries installed\")\n",
    "\n",
    "# Set to True to run the demo\n",
    "run_rag_demo = True\n",
    "\n",
    "if run_rag_demo:\n",
    "    demo_rag_system()\n",
    "else:\n",
    "    print(\"üéØ RAG SYSTEM DEMO READY\")\n",
    "    print(\"Set run_rag_demo = True to see the complete RAG system in action!\")\n",
    "    print()\n",
    "    print(\"This demo will show:\")\n",
    "    print(\"1. üß† Intelligent query parsing (extracts entities from natural language)\")\n",
    "    print(\"2. üîç Embedding-based semantic search\")\n",
    "    print(\"3. üìä Content statistics and result analysis\")\n",
    "    print(\"4. üéØ Category-filtered search\")\n",
    "    print(\"5. üìù Detailed results with similarity scores\")\n",
    "    print()\n",
    "    print(\"Example queries that will work:\")\n",
    "    print(\"‚Ä¢ 'Find mountain bikes with black color'\")\n",
    "    print(\"‚Ä¢ 'Show me technical manuals'\") \n",
    "    print(\"‚Ä¢ 'What bike components are available?'\")\n",
    "    print(\"‚Ä¢ 'Find products similar to size 48 frames'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0214ae03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "üìÇ Loading embeddings from ../data/knowledge_graph_embeddings.pkl\n",
      "‚úÖ Loaded 579 embeddings\n",
      "=== SEMANTIC SEARCH DEMO ===\n",
      "\n",
      "üìä Database contains 579 items:\n",
      "   database_table: 573\n",
      "   json_table: 6\n",
      "\n",
      "üß† Embedding dimension: 384\n",
      "\n",
      "============================================================\n",
      "üîç QUERY: 'mountain bike frame'\n",
      "============================================================\n",
      "üîç Searching for: 'mountain bike frame'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108d87936c0845a5a411520f72193842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Rank 1 (similarity: 0.607)\n",
      "   Type: database_table\n",
      "   Source: ProductDescription\n",
      "   Text: Table: ProductDescription. Description: Lightweight butted aluminum frame provides a more upright riding position for a trip around town.  Our ground-breaking design provides optimum comfort.. Modifie...\n",
      "\n",
      "üèÜ Rank 2 (similarity: 0.595)\n",
      "   Type: database_table\n",
      "   Source: ProductDescription\n",
      "   Text: Table: ProductDescription. Description: Top-of-the-line competition mountain bike. Performance-enhancing options include the innovative HL Frame, super-smooth front suspension, and traction for all te...\n",
      "\n",
      "üèÜ Rank 3 (similarity: 0.550)\n",
      "   Type: database_table\n",
      "   Source: ProductDescription\n",
      "   Text: Table: ProductDescription. Description: Each frame is hand-crafted in our Bothell facility to the optimum diameter and wall-thickness required of a premium mountain frame. The heat-treated welded alum...\n",
      "\n",
      "============================================================\n",
      "üîç QUERY: 'red helmet'\n",
      "============================================================\n",
      "üîç Searching for: 'red helmet'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749e115f679149f495899821f2f81523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Rank 1 (similarity: 0.546)\n",
      "   Type: database_table\n",
      "   Source: Product\n",
      "   Text: Table: Product. Name: Sport-100 Helmet, Red. ProductNumber: HL-U509-R. Color: Red. SellStartDate: 2005-07-01 00:00:00.000. ThumbNailPhoto: 0x47494638396150003100F70000000000800000008000808000000080800...\n",
      "\n",
      "üèÜ Rank 2 (similarity: 0.516)\n",
      "   Type: database_table\n",
      "   Source: Product\n",
      "   Text: Table: Product. Name: Sport-100 Helmet, Black. ProductNumber: HL-U509. Color: Black. SellStartDate: 2005-07-01 00:00:00.000. ThumbNailPhoto: 0x47494638396150003100F700000000008000000080008080000000808...\n",
      "\n",
      "üèÜ Rank 3 (similarity: 0.509)\n",
      "   Type: database_table\n",
      "   Source: Product\n",
      "   Text: Table: Product. Name: Sport-100 Helmet, Blue. ProductNumber: HL-U509-B. Color: Blue. SellStartDate: 2005-07-01 00:00:00.000. ThumbNailPhoto: 0x47494638396150003100F700000000008000000080008080000000808...\n",
      "\n",
      "============================================================\n",
      "üîç QUERY: 'road cycling equipment'\n",
      "============================================================\n",
      "üîç Searching for: 'road cycling equipment'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346993586bf44cbb9d830ee38ed44fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Rank 1 (similarity: 0.583)\n",
      "   Type: database_table\n",
      "   Source: ProductDescription\n",
      "   Text: Table: ProductDescription. Description: Same technology as all of our Road series bikes.  Perfect all-around bike for road or racing.. ModifiedDate: 2007-06-01 00:00:00.000. ProductDescriptionID: 321...\n",
      "\n",
      "üèÜ Rank 2 (similarity: 0.541)\n",
      "   Type: database_table\n",
      "   Source: ProductDescription\n",
      "   Text: Table: ProductDescription. Description: Suitable for any type of riding, on or off-road. Fits any budget. Smooth-shifting with a comfortable ride.. ModifiedDate: 2007-06-01 00:00:00.000. ProductDescri...\n",
      "\n",
      "üèÜ Rank 3 (similarity: 0.509)\n",
      "   Type: database_table\n",
      "   Source: ProductDescription\n",
      "   Text: Table: ProductDescription. Description: Travel in style and comfort. Designed for maximum comfort and safety. Wide gear range takes on all hills. High-tech aluminum alloy construction provides durabil...\n",
      "\n",
      "============================================================\n",
      "üîç QUERY: 'vintage bicycle parts'\n",
      "============================================================\n",
      "üîç Searching for: 'vintage bicycle parts'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c276b22224094e0692ccec130cf018bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Rank 1 (similarity: 0.484)\n",
      "   Type: database_table\n",
      "   Source: ProductDescription\n",
      "   Text: Table: ProductDescription. Description: Clipless pedals - aluminum.. ModifiedDate: 2007-06-01 00:00:00.000. ProductDescriptionID: 850...\n",
      "\n",
      "üèÜ Rank 2 (similarity: 0.458)\n",
      "   Type: database_table\n",
      "   Source: ProductDescription\n",
      "   Text: Table: ProductDescription. Description: Value-priced bike with many features of our top-of-the-line models. Has the same light, stiff frame, and the quick acceleration we're famous for.. ModifiedDate:...\n",
      "\n",
      "üèÜ Rank 3 (similarity: 0.457)\n",
      "   Type: database_table\n",
      "   Source: ProductDescription\n",
      "   Text: Table: ProductDescription. Description: Replacement rear wheel for entry-level cyclist.. ModifiedDate: 2007-06-01 00:00:00.000. ProductDescriptionID: 870...\n",
      "\n",
      "============================================================\n",
      "üîó FINDING SIMILAR ITEMS\n",
      "============================================================\n",
      "üîç Finding items similar to: Table: Product. Name: HL Road Frame - Black, 58. ProductNumber: FR-R92B-58. Color: Black. Size: 58. ...\n",
      "\n",
      "üîó Similarity: 0.989\n",
      "   Type: database_table\n",
      "   Text: Table: Product. Name: HL Road Frame - Red, 58. ProductNumber: FR-R92R-58. Color: Red. Size: 58. SellStartDate: 2002-06-01 00:00:00.000. ThumbNailPhoto: 0x47494638396150003100F7000000000080000000800080...\n",
      "\n",
      "üîó Similarity: 0.985\n",
      "   Type: database_table\n",
      "   Text: Table: Product. Name: HL Road Frame - Red, 56. ProductNumber: FR-R92R-56. Color: Red. Size: 56. SellStartDate: 2005-07-01 00:00:00.000. ThumbNailPhoto: 0x47494638396150003100F7000000000080000000800080...\n",
      "\n",
      "üîó Similarity: 0.985\n",
      "   Type: database_table\n",
      "   Text: Table: Product. Name: HL Road Frame - Red, 52. ProductNumber: FR-R92R-52. Color: Red. Size: 52. SellStartDate: 2005-07-01 00:00:00.000. ThumbNailPhoto: 0x47494638396150003100F7000000000080000000800080...\n"
     ]
    }
   ],
   "source": [
    "# üéØ RUN SEMANTIC SEARCH DEMO\n",
    "# This demonstrates how to use the embeddings for intelligent search\n",
    "\n",
    "# Set to True to run the demo\n",
    "run_demo = True\n",
    "\n",
    "if run_demo:\n",
    "    demo_semantic_search()\n",
    "else:\n",
    "    print(\"üéØ SEMANTIC SEARCH DEMO READY\")\n",
    "    print(\"Set run_demo = True to see the search engine in action!\")\n",
    "    print()\n",
    "    print(\"What this demo will show:\")\n",
    "    print(\"1. üìä Statistics about your embedded knowledge base\")\n",
    "    print(\"2. üîç Semantic search examples (find items by natural language)\")\n",
    "    print(\"3. üîó Similarity search (find items similar to a specific one)\")\n",
    "    print(\"4. üéØ Filtered search (e.g., only database tables or only JSON files)\")\n",
    "    print()\n",
    "    print(\"Example queries that will work:\")\n",
    "    print(\"‚Ä¢ 'mountain bike frame' - finds bike-related products\")\n",
    "    print(\"‚Ä¢ 'red helmet' - finds red-colored safety equipment\")\n",
    "    print(\"‚Ä¢ 'vintage bicycle parts' - finds historical/vintage items\")\n",
    "    print(\"‚Ä¢ 'technical specifications' - finds detailed product info\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a915d648",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
